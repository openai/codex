---
name: codex-cli-docs
description: Codex CLI documentation bundle (docs map + repo docs).
metadata:
  short-description: Codex CLI docs bundle
---

# Codex CLI docs bundle (embedded)

This file is auto-generated by `scripts/generate_codex_cli_docs_map.py` and is installed as a system skill by default.

h/t Simon Willison for the docs map pattern: https://simonwillison.net/2025/Oct/24/claude-code-docs-map/

## Docs map

# Codex CLI Documentation Map

This is a comprehensive map of Codex CLI documentation pages with their headings, designed for easy navigation by LLMs.

> **Note:** This file is auto-generated by `scripts/generate_codex_cli_docs_map.py`. Do not edit manually.

## Document Structure

This map uses a hierarchical structure:

* **##** marks documentation groups
* **###** marks individual documentation pages
* Nested bullets show the heading structure within each page
* Each page title links to the full documentation

## Docs

### [CLA](./CLA.md)

* 1. Definitions
* 2. Copyright License
* 3. Patent License
* 4. Representations
* 5. Miscellany

### [agents_md](./agents_md.md)

* (no headings)

### [authentication](./authentication.md)

* (no headings)

### [config](./config.md)

* Connecting to MCP servers
* Notify

### [contributing](./contributing.md)

* Contributing
  * Development workflow
  * Writing high-impact code changes
  * Opening a pull request
  * Review process
  * Community values
  * Getting help
  * Contributor license agreement (CLA)
  * Security & responsible AI

### [example-config](./example-config.md)

* (no headings)

### [exec](./exec.md)

* (no headings)

### [execpolicy](./execpolicy.md)

* (no headings)

### [getting-started](./getting-started.md)

* (no headings)

### [install](./install.md)

* Installing & building
  * System requirements
  * DotSlash
  * Build from source
* Tracing / verbose logging

### [license](./license.md)

* License

### [open-source-fund](./open-source-fund.md)

* Codex open source fund

### [prompts](./prompts.md)

* (no headings)

### [sandbox](./sandbox.md)

* Sandbox & approvals

### [skills](./skills.md)

* (no headings)

### [slash_commands](./slash_commands.md)

* (no headings)

### [telemetry](./telemetry.md)

* Config
* Tracing
* Feedback
* Metrics
  * Global context (applies to every event/metric)
* Metrics catalog
  * Metrics to be added

## TUI2

### [tui2/performance-testing](./tui2/performance-testing.md)

* Scope (this round)
* Roles
* Baseline setup
* Capture profiles (macOS)
* Quick hotspot extraction
* Rasterization-cache validation checklist
* Notes to record per run
* Code pointers

## Repo docs

### docs/CLA.md

# Individual Contributor License Agreement (v1.0, OpenAI)

_Based on the Apache Software Foundation Individual CLA v 2.2._

By commenting **“I have read the CLA Document and I hereby sign the CLA”**
on a Pull Request, **you (“Contributor”) agree to the following terms** for any
past and future “Contributions” submitted to the **OpenAI Codex CLI project
(the “Project”)**.

---

## 1. Definitions

- **“Contribution”** – any original work of authorship submitted to the Project
  (code, documentation, designs, etc.).
- **“You” / “Your”** – the individual (or legal entity) posting the acceptance
  comment.

## 2. Copyright License

You grant **OpenAI, Inc.** and all recipients of software distributed by the
Project a perpetual, worldwide, non‑exclusive, royalty‑free, irrevocable
license to reproduce, prepare derivative works of, publicly display, publicly
perform, sublicense, and distribute Your Contributions and derivative works.

## 3. Patent License

You grant **OpenAI, Inc.** and all recipients of the Project a perpetual,
worldwide, non‑exclusive, royalty‑free, irrevocable (except as below) patent
license to make, have made, use, sell, offer to sell, import, and otherwise
transfer Your Contributions alone or in combination with the Project.

If any entity brings patent litigation alleging that the Project or a
Contribution infringes a patent, the patent licenses granted by You to that
entity under this CLA terminate.

## 4. Representations

1. You are legally entitled to grant the licenses above.
2. Each Contribution is either Your original creation or You have authority to
   submit it under this CLA.
3. Your Contributions are provided **“AS IS”** without warranties of any kind.
4. You will notify the Project if any statement above becomes inaccurate.

## 5. Miscellany

This Agreement is governed by the laws of the **State of California**, USA,
excluding its conflict‑of‑laws rules. If any provision is held unenforceable,
the remaining provisions remain in force.

### docs/agents_md.md

# AGENTS.md

For information about AGENTS.md, see [this documentation](https://developers.openai.com/codex/guides/agents-md).

### docs/authentication.md

# Authentication

For information about Codex CLI authentication, see [this documentation](https://developers.openai.com/codex/auth).

### docs/config.md

# Configuration

For basic configuration instructions, see [this documentation](https://developers.openai.com/codex/config-basic).

For advanced configuration instructions, see [this documentation](https://developers.openai.com/codex/config-advanced).

For a full configuration reference, see [this documentation](https://developers.openai.com/codex/config-reference).

## Connecting to MCP servers

Codex can connect to MCP servers configured in `~/.codex/config.toml`. See the configuration reference for the latest MCP server options:

- https://developers.openai.com/codex/config-reference

## Notify

Codex can run a notification hook when the agent finishes a turn. See the configuration reference for the latest notification settings:

- https://developers.openai.com/codex/config-reference

### docs/contributing.md

## Contributing

This project is under active development and the code will likely change pretty significantly.

**At the moment, we are generally accepting external contributions only for bugs fixes.**

If you want to add a new feature or change the behavior of an existing one, please open an issue proposing the feature or upvote an existing enhancement request. We will generally prioritize new features based on community feedback. New features must compose well with existing and upcoming features and fit into our roadmap. They must also be implemented consistently across all Codex surfaces (CLI, IDE extension, web, etc.).

If you want to contribute a bug fix, please open a bug report first - or verify that there is an existing bug report that discusses the issue. All bug fix PRs should include a link to a bug report.

**New contributions that don't go through this process may be closed** if they aren't aligned with our current roadmap or conflict with other priorities/upcoming features.

### Development workflow

- Create a _topic branch_ from `main` - e.g. `feat/interactive-prompt`.
- Keep your changes focused. Multiple unrelated fixes should be opened as separate PRs.
- Ensure your change is free of lint warnings and test failures.

### Writing high-impact code changes

1. **Start with an issue.** Open a new one or comment on an existing discussion so we can agree on the solution before code is written.
2. **Add or update tests.** A bug fix should generally come with test coverage that fails before your change and passes afterwards. 100% coverage is not required, but aim for meaningful assertions.
3. **Document behavior.** If your change affects user-facing behavior, update the README, inline help (`codex --help`), or relevant example projects.
4. **Keep commits atomic.** Each commit should compile and the tests should pass. This makes reviews and potential rollbacks easier.

### Opening a pull request

- Fill in the PR template (or include similar information) - **What? Why? How?**
- Include a link to a bug report or enhancement request in the issue tracker
- Run **all** checks locally. Use the root `just` helpers so you stay consistent with the rest of the workspace: `just fmt`, `just fix -p <crate>` for the crate you touched, and the relevant tests (e.g., `cargo test -p codex-tui` or `just test` if you need a full sweep). CI failures that could have been caught locally slow down the process.
- Make sure your branch is up-to-date with `main` and that you have resolved merge conflicts.
- Mark the PR as **Ready for review** only when you believe it is in a merge-able state.

### Review process

1. One maintainer will be assigned as a primary reviewer.
2. If your PR adds a new feature that was not previously discussed and approved, we may close your PR (see [Contributing](#contributing)).
3. We may ask for changes. Please do not take this personally. We value the work, but we also value consistency and long-term maintainability.
4. When there is consensus that the PR meets the bar, a maintainer will squash-and-merge.

### Community values

- **Be kind and inclusive.** Treat others with respect; we follow the [Contributor Covenant](https://www.contributor-covenant.org/).
- **Assume good intent.** Written communication is hard - err on the side of generosity.
- **Teach & learn.** If you spot something confusing, open an issue or PR with improvements.

### Getting help

If you run into problems setting up the project, would like feedback on an idea, or just want to say _hi_ - please open a Discussion topic or jump into the relevant issue. We are happy to help.

Together we can make Codex CLI an incredible tool. **Happy hacking!** :rocket:

### Contributor license agreement (CLA)

All contributors **must** accept the CLA. The process is lightweight:

1. Open your pull request.
2. Paste the following comment (or reply `recheck` if you've signed before):

   ```text
   I have read the CLA Document and I hereby sign the CLA
   ```

3. The CLA-Assistant bot records your signature in the repo and marks the status check as passed.

No special Git commands, email attachments, or commit footers required.

### Security & responsible AI

Have you discovered a vulnerability or have concerns about model output? Please e-mail **security@openai.com** and we will respond promptly.

### docs/example-config.md

# Sample configuration

For a sample configuration file, see [this documentation](https://developers.openai.com/codex/config-sample).

### docs/exec.md

# Non-interactive mode

For information about non-interactive mode, see [this documentation](https://developers.openai.com/codex/noninteractive).

### docs/execpolicy.md

# Execution policy

For an overview of execution policy rules, see [this documentation](https://developers.openai.com/codex/exec-policy).

### docs/getting-started.md

# Getting started with Codex CLI

For an overview of Codex CLI features, see [this documentation](https://developers.openai.com/codex/cli/features#running-in-interactive-mode).

### docs/install.md

## Installing & building

### System requirements

| Requirement                 | Details                                                         |
| --------------------------- | --------------------------------------------------------------- |
| Operating systems           | macOS 12+, Ubuntu 20.04+/Debian 10+, or Windows 11 **via WSL2** |
| Git (optional, recommended) | 2.23+ for built-in PR helpers                                   |
| RAM                         | 4-GB minimum (8-GB recommended)                                 |

### DotSlash

The GitHub Release also contains a [DotSlash](https://dotslash-cli.com/) file for the Codex CLI named `codex`. Using a DotSlash file makes it possible to make a lightweight commit to source control to ensure all contributors use the same version of an executable, regardless of what platform they use for development.

### Build from source

```bash
# Clone the repository and navigate to the root of the Cargo workspace.
git clone https://github.com/openai/codex.git
cd codex/codex-rs

# Install the Rust toolchain, if necessary.
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
source "$HOME/.cargo/env"
rustup component add rustfmt
rustup component add clippy
# Install helper tools used by the workspace justfile:
cargo install just
# Optional: install nextest for the `just test` helper
cargo install cargo-nextest

# Build Codex.
cargo build

# Launch the TUI with a sample prompt.
cargo run --bin codex -- "explain this codebase to me"

# After making changes, use the root justfile helpers (they default to codex-rs):
just fmt
just fix -p <crate-you-touched>

# Run the relevant tests (project-specific is fastest), for example:
cargo test -p codex-tui
# If you have cargo-nextest installed, `just test` runs the test suite via nextest:
just test
# If you specifically want the full `--all-features` matrix, use:
cargo test --all-features
```

## Tracing / verbose logging

Codex is written in Rust, so it honors the `RUST_LOG` environment variable to configure its logging behavior.

The TUI defaults to `RUST_LOG=codex_core=info,codex_tui=info,codex_rmcp_client=info` and log messages are written to `~/.codex/log/codex-tui.log`, so you can leave the following running in a separate terminal to monitor log messages as they are written:

```bash
tail -F ~/.codex/log/codex-tui.log
```

By comparison, the non-interactive mode (`codex exec`) defaults to `RUST_LOG=error`, but messages are printed inline, so there is no need to monitor a separate file.

See the Rust documentation on [`RUST_LOG`](https://docs.rs/env_logger/latest/env_logger/#enabling-logging) for more information on the configuration options.

### docs/license.md

## License

This repository is licensed under the [Apache-2.0 License](../LICENSE).

### docs/open-source-fund.md

## Codex open source fund

We're excited to launch a **$1 million initiative** supporting open source projects that use Codex CLI and other OpenAI models.

- Grants are awarded up to **$25,000** API credits.
- Applications are reviewed **on a rolling basis**.

**Interested? [Apply here](https://openai.com/form/codex-open-source-fund/).**

### docs/prompts.md

# Custom prompts

For an overview of custom prompts, see [this documentation](https://developers.openai.com/codex/custom-prompts).

### docs/sandbox.md

## Sandbox & approvals

For information about Codex sandboxing and approvals, see [this documentation](https://developers.openai.com/codex/security).

### docs/skills.md

# Skills

For information about skills, refer to [this documentation](https://developers.openai.com/codex/skills).

### docs/slash_commands.md

# Slash commands

For an overview of Codex CLI slash commands, see [this documentation](https://developers.openai.com/codex/cli/slash-commands).

### docs/telemetry.md

# Codex Telemetry

## Config

**TODO(jif)**: add the config and document it

## Tracing

Codex can export OpenTelemetry **log events**, **trace spans**, and **metrics**
when OTEL exporters are configured in `config.toml` (`[otel]`).
By default, exporters are disabled and nothing is sent.

## Feedback

Feedback is sent only when you run `/feedback` and confirm. The report includes
the selected category and optional note; if you opt in to include logs, Codex
attaches the most recent in-memory logs for the session (up to ~4 MiB).

## Metrics

This section list all the metrics exported by Codex when locally installed.

### Global context (applies to every event/metric)

- `surface`: `cli` | `vscode` | `exec` | `mcp` | `subagent_*` (from `SessionSource`).
- `version`: binary version.
- `auth_mode`: `swic` (AuthMode::ChatGPT) | `api` (AuthMode::ApiKey) | `unknown`.
- `model`: name of the model used.

## Metrics catalog

Each metric includes the required fields plus the global context above. Every metrics are prefixed by `codex.`.

| Metric            | Type    | Fields         | Description                                                              |
| ----------------- | ------- | -------------- | ------------------------------------------------------------------------ |
| `features.state`  | counter | `key`, `value` | Feature values that differ from defaults (emit one row per non-default). |
| `session.started` | counter | `is_git`       | New session created.                                                     |
| `task.compact`    | counter | `type`         | Number of compaction per type (`remote` or `local`)                      |
| `task.user_shell` | counter |                | Number of user shell actions (`!` in the TUI for example)                |
| `task.review`     | counter |                | Number of reviews triggered                                              |
| `task.undo`       | counter |                | Number of undo made                                                      |

### Metrics to be added

| Metric                    | Type      | Fields                                | Description                                               |
| ------------------------- | --------- | ------------------------------------- | --------------------------------------------------------- |
| `approval.requested`      | counter   | `tool`, `approved`                    | Tool approval request result (`approved`: `yes` or `no`). |
| `conversation.turn.count` | counter   |                                       | User/assistant turns per session.                         |
| `mcp.call`                | counter   | `status`                              | MCP tool invocation result (`ok` or error string).        |
| `model.call.duration_ms`  | histogram | `provider`, `status`, `attempt`       | Model API request duration.                               |
| `tool.call`               | counter   | `tool`, `status`                      | Tool invocation result (`ok` or error string).            |
| `tool.call.duration_ms`   | histogram | `tool`, `status`                      | Tool execution time.                                      |
| `user.feedback.submitted` | counter   | `category`, `include_logs`, `success` | Feedback submission via `/feedback`.                      |

### docs/tui2/performance-testing.md

# Performance testing (`codex-tui2`)

This doc captures a repeatable workflow for investigating `codex-tui2` performance issues
(especially high idle CPU and high CPU while streaming) and validating optimizations to the draw
hot path.

## Scope (this round)

The current focus is the transcript draw hot path, specifically the cost of repeatedly rendering
the same visible transcript lines via Ratatui’s `Line::render_ref` (notably grapheme segmentation
and span layout).

The intended mitigation is a **rasterization cache**: render a wrapped transcript `Line` into a
row of `Cell`s once, cache it, and on subsequent redraws copy cached cells into the frame buffer.

Key invariants:

- The cache is width-scoped (invalidate on terminal width changes).
- The cache stores **base content** only; selection highlight and copy affordances are applied
  after rendering, so they don’t pollute cached rows.

## Roles

- Human: runs `codex-tui2` in an interactive terminal (e.g. Ghostty), triggers “idle” and
  “streaming” scenarios, and captures profiles.
- Assistant (or a script): reads profile output and extracts hotspots and deltas.

## Baseline setup

Build from a clean checkout:

```sh
cd codex-rs
cargo build -p codex-tui2
```

Run `codex-tui2` in a terminal and get a PID (macOS):

```sh
pgrep -n codex-tui2
```

Track CPU quickly while reproducing:

```sh
top -pid "$(pgrep -n codex-tui2)"
```

## Capture profiles (macOS)

Capture both an “idle” and a “streaming” profile so hotspots are not conflated:

```sh
sample "$(pgrep -n codex-tui2)" 1 -file /tmp/tui2.idle.sample.txt
sample "$(pgrep -n codex-tui2)" 1 -file /tmp/tui2.streaming.sample.txt
```

For the streaming sample, trigger a response that emits many deltas (e.g. “Tell me a story”) so
the stream runs long enough to sample.

## Quick hotspot extraction

These `rg` patterns keep the investigation grounded in the data:

```sh
# Buffer diff hot path (idle)
rg -n "custom_terminal::diff_buffers|diff_buffers" /tmp/tui2.*.sample.txt | head -n 80

# Transcript rendering hot path (streaming)
rg -n "App::render_transcript_cells|Line::render|render_spans|styled_graphemes|GraphemeCursor::next_boundary" /tmp/tui2.*.sample.txt | head -n 120
```

## Rasterization-cache validation checklist

After implementing a transcript rasterization cache, re-run the same scenarios and confirm:

- Streaming sample shifts away from `unicode_segmentation::grapheme::GraphemeCursor::next_boundary`
  stacks dominating the main thread.
- CPU during streaming drops materially vs baseline for the same streaming load.
- Idle CPU does not regress (redraw gating changes can mask rendering improvements; always measure
  both idle and streaming).

## Notes to record per run

- Terminal size: width × height
- Scenario: idle vs streaming (prompt + approximate response length)
- CPU snapshot: `top` (directional)
- Profile excerpt: 20–50 relevant lines for the dominant stacks

## Code pointers

- `codex-rs/tui2/src/transcript_view_cache.rs`: wrapped transcript memoization + per-line
  rasterization cache (cached `Cell` rows).
- `codex-rs/tui2/src/transcript_render.rs`: incremental helper used by the wrapped-line cache
  (`append_wrapped_transcript_cell`).
- `codex-rs/tui2/src/app.rs`: wiring in `App::render_transcript_cells` (uses cached rows instead of
  calling `Line::render_ref` every frame).
