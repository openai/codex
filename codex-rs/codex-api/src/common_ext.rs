//! Non-streaming response handling and tweakcc input filtering utilities.
//!
//! This module provides:
//! - `NonStreamingResponse` type for complete API responses
//! - Response parsing from JSON (based on gpt_adapter.rs)
//! - Incremental input filtering for `previous_response_id` support (based on item_utils.rs)

use crate::common::ResponseEvent;
use crate::error::ApiError;
use codex_protocol::models::ContentItem;
use codex_protocol::models::ReasoningItemContent;
use codex_protocol::models::ReasoningItemReasoningSummary;
use codex_protocol::models::ResponseItem;
use codex_protocol::protocol::TokenUsage;
use serde::Deserialize;
use serde::Serialize;
use serde_json::Value;
use tokio::sync::mpsc;

/// Complete response from non-streaming API call.
#[derive(Debug)]
pub struct NonStreamingResponse {
    /// Response events (OutputItemDone, Completed, etc.)
    pub events: Vec<ResponseEvent>,
    /// Response ID from the API
    pub response_id: String,
    /// Token usage statistics
    pub token_usage: Option<TokenUsage>,
}

impl NonStreamingResponse {
    /// Convert to `ResponseStream` for compatibility with streaming API consumers.
    pub fn into_stream(self) -> crate::common::ResponseStream {
        let (tx, rx) = mpsc::channel(self.events.len() + 1);
        tokio::spawn(async move {
            for event in self.events {
                let _ = tx.send(Ok(event)).await;
            }
        });
        crate::common::ResponseStream { rx_event: rx }
    }
}

// =============================================================================
// Incremental Input Filtering (from core/src/adapters/item_utils.rs)
// =============================================================================

/// Determine if a `ResponseItem` was generated by the LLM.
///
/// LLM-generated items include:
/// - Assistant messages
/// - Reasoning items
/// - Function calls (tool calls requested by the model)
/// - Custom tool calls
/// - Local shell calls
/// - Web search calls
///
/// User-provided items include:
/// - User messages
/// - Function call outputs (tool results)
/// - Custom tool call outputs
/// - Ghost snapshots
/// - Compaction summaries
pub fn is_llm_generated(item: &ResponseItem) -> bool {
    match item {
        // LLM outputs (server has via previous_response_id)
        ResponseItem::Message { role, .. } if role == "assistant" => true,
        ResponseItem::Reasoning { .. } => true,
        ResponseItem::FunctionCall { .. } => true,
        ResponseItem::CustomToolCall { .. } => true,
        ResponseItem::LocalShellCall { .. } => true,
        ResponseItem::WebSearchCall { .. } => true,

        // User inputs (server needs in request)
        ResponseItem::FunctionCallOutput { .. } => false,
        ResponseItem::CustomToolCallOutput { .. } => false,
        ResponseItem::Message { role, .. } if role == "user" => false,
        ResponseItem::GhostSnapshot { .. } => false,
        ResponseItem::Compaction { .. } => false,
        ResponseItem::Other => false,

        // Edge case: message with unknown role (defensive)
        ResponseItem::Message { .. } => false,
    }
}

/// Filter input to get items after last LLM-generated item (zero-copy).
///
/// Used for tweakcc mode when `previous_response_id` is present.
/// The server already has history up to the last LLM response, so we only
/// need to send user inputs that occurred after that point.
///
/// # Returns
///
/// - `None` - No LLM items found (first turn, caller should use full input)
/// - `Some(&[])` - LLM item is last (error state, caller should return error)
/// - `Some(&[...])` - Items after last LLM item (normal case)
///
/// # Example
///
/// ```ignore
/// match filter_incremental_input(&prompt.input) {
///     None => /* first turn, use full input */,
///     Some(slice) if slice.is_empty() => /* error: no user input after LLM */,
///     Some(slice) => /* use filtered slice */,
/// }
/// ```
pub fn filter_incremental_input(full_input: &[ResponseItem]) -> Option<&[ResponseItem]> {
    let last_llm_idx = full_input.iter().rposition(is_llm_generated)?;
    Some(&full_input[last_llm_idx + 1..])
}

// =============================================================================
// Response Parsing (from core/src/adapters/gpt_openapi/gpt_adapter.rs)
// =============================================================================

/// Token usage details for complete (non-streaming) responses.
#[derive(Debug, Deserialize)]
struct CompleteResponseUsage {
    input_tokens: i64,
    input_tokens_details: Option<CompleteResponseInputTokensDetails>,
    output_tokens: i64,
    output_tokens_details: Option<CompleteResponseOutputTokensDetails>,
}

/// Input token details for complete responses.
#[derive(Debug, Deserialize)]
struct CompleteResponseInputTokensDetails {
    cached_tokens: i64,
}

/// Output token details for complete responses.
#[derive(Debug, Deserialize)]
struct CompleteResponseOutputTokensDetails {
    reasoning_tokens: i64,
}

/// Parse complete (non-streaming) Responses API JSON response.
///
/// Converts a complete JSON response body to `NonStreamingResponse`.
/// Based on `gpt_adapter.rs::parse_complete_responses_json()`.
pub fn parse_complete_response(body: &str) -> Result<NonStreamingResponse, ApiError> {
    let data: Value =
        serde_json::from_str(body).map_err(|e| ApiError::Stream(format!("Invalid JSON: {e}")))?;

    let mut events = Vec::new();

    // Extract response ID
    let response_id = data
        .get("id")
        .and_then(|i| i.as_str())
        .unwrap_or("")
        .to_string();

    tracing::debug!(response_id = %response_id, "Parsing non-streaming response");

    // Check status field (must be present and valid)
    let Some(status) = data.get("status").and_then(|s| s.as_str()) else {
        return Err(ApiError::Stream("Missing status field in response".into()));
    };

    // Handle different status values
    match status {
        "completed" => {
            // Continue with normal processing
        }
        "failed" => {
            if let Some(error) = data.get("error") {
                return Err(classify_error(error));
            }
            return Err(ApiError::Stream(
                "Response failed without error details".into(),
            ));
        }
        "incomplete" => {
            let reason = data
                .get("incomplete_details")
                .and_then(|d| d.get("reason"))
                .and_then(|r| r.as_str())
                .unwrap_or("unknown");

            return Err(ApiError::Stream(format!("Response incomplete: {reason}")));
        }
        _ => {
            // Unknown status - reject it
            return Err(ApiError::Stream(format!(
                "Unknown response status: {status}"
            )));
        }
    }

    // Parse output items
    if let Some(output_array) = data.get("output").and_then(|o| o.as_array()) {
        for item_data in output_array {
            if let Some(item) = parse_output_item(item_data)? {
                events.push(ResponseEvent::OutputItemDone(item));
            }
        }
    }

    // Parse token usage with proper nested structure handling
    let token_usage = parse_token_usage(data.get("usage"));

    // Add Completed event
    events.push(ResponseEvent::Completed {
        response_id: response_id.clone(),
        token_usage: token_usage.clone(),
    });

    Ok(NonStreamingResponse {
        events,
        response_id,
        token_usage,
    })
}

/// Parse a single output item from complete response.
///
/// Returns `None` if item type is not recognized or parsing fails silently.
fn parse_output_item(item_data: &Value) -> Result<Option<ResponseItem>, ApiError> {
    tracing::debug!(
        item_data = ?item_data,
        "Parsing output item from response"
    );

    let item_type = item_data.get("type").and_then(|t| t.as_str()).unwrap_or("");

    match item_type {
        "message" => {
            let id = item_data
                .get("id")
                .and_then(|i| i.as_str())
                .map(std::string::ToString::to_string);

            let role = item_data
                .get("role")
                .and_then(|r| r.as_str())
                .unwrap_or("assistant")
                .to_string();

            // Parse content array
            let mut content = Vec::new();
            if let Some(content_array) = item_data.get("content").and_then(|c| c.as_array()) {
                for content_item in content_array {
                    let content_type = content_item
                        .get("type")
                        .and_then(|t| t.as_str())
                        .unwrap_or("");

                    match content_type {
                        "output_text" => {
                            if let Some(text) = content_item.get("text").and_then(|t| t.as_str()) {
                                content.push(ContentItem::OutputText {
                                    text: text.to_string(),
                                });
                            }
                        }
                        _ => {
                            // Skip unknown content types
                        }
                    }
                }
            }

            Ok(Some(ResponseItem::Message { id, role, content, end_turn: None }))
        }

        "function_call" => {
            let id = item_data
                .get("id")
                .and_then(|i| i.as_str())
                .map(std::string::ToString::to_string);

            let name = item_data
                .get("name")
                .and_then(|n| n.as_str())
                .unwrap_or("")
                .to_string();

            let call_id = item_data
                .get("call_id")
                .and_then(|c| c.as_str())
                .unwrap_or("")
                .to_string();

            let arguments = item_data
                .get("arguments")
                .and_then(|a| a.as_str())
                .unwrap_or("{}")
                .to_string();

            Ok(Some(ResponseItem::FunctionCall {
                id,
                name,
                call_id,
                arguments,
            }))
        }

        "reasoning" => {
            let id = item_data
                .get("id")
                .and_then(|i| i.as_str())
                .unwrap_or("")
                .to_string();

            // Parse summary array
            let mut summary = Vec::new();
            if let Some(summary_array) = item_data.get("summary").and_then(|s| s.as_array()) {
                for summary_item in summary_array {
                    if let Some(text) = summary_item.get("text").and_then(|t| t.as_str()) {
                        summary.push(ReasoningItemReasoningSummary::SummaryText {
                            text: text.to_string(),
                        });
                    }
                }
            }

            // Parse content array (optional)
            let content =
                if let Some(content_array) = item_data.get("content").and_then(|c| c.as_array()) {
                    let mut content_items = Vec::new();
                    for content_item in content_array {
                        if let Some(text) = content_item.get("text").and_then(|t| t.as_str()) {
                            content_items.push(ReasoningItemContent::ReasoningText {
                                text: text.to_string(),
                            });
                        }
                    }
                    if content_items.is_empty() {
                        None
                    } else {
                        Some(content_items)
                    }
                } else {
                    None
                };

            let encrypted_content = item_data
                .get("encrypted_content")
                .and_then(|e| e.as_str())
                .map(std::string::ToString::to_string);

            Ok(Some(ResponseItem::Reasoning {
                id,
                summary,
                content,
                encrypted_content,
            }))
        }

        _ => {
            // Unknown item type, skip it
            Ok(None)
        }
    }
}

/// Parse token usage from response JSON.
fn parse_token_usage(usage: Option<&Value>) -> Option<TokenUsage> {
    usage.and_then(|u| {
        serde_json::from_value::<CompleteResponseUsage>(u.clone())
            .ok()
            .map(|usage| TokenUsage {
                input_tokens: usage.input_tokens,
                cached_input_tokens: usage
                    .input_tokens_details
                    .map(|d| d.cached_tokens)
                    .unwrap_or(0),
                output_tokens: usage.output_tokens,
                reasoning_output_tokens: usage
                    .output_tokens_details
                    .map(|d| d.reasoning_tokens)
                    .unwrap_or(0),
                total_tokens: usage.input_tokens + usage.output_tokens,
            })
    })
}

/// Classify error from response JSON into appropriate `ApiError`.
fn classify_error(error: &Value) -> ApiError {
    let code = error.get("code").and_then(|c| c.as_str()).unwrap_or("");
    let message = error
        .get("message")
        .and_then(|m| m.as_str())
        .unwrap_or("Unknown error");

    // Check message for encrypted content verification error
    // OpenAI returns: "The encrypted content for item rs_xxx could not be verified."
    if message.contains("encrypted content") && message.contains("could not be verified") {
        return ApiError::EncryptedContentInvalid;
    }

    match code {
        "context_length_exceeded" => ApiError::ContextWindowExceeded,
        "insufficient_quota" => ApiError::QuotaExceeded,
        "previous_response_not_found" => ApiError::PreviousResponseNotFound,
        _ => ApiError::Stream(message.to_string()),
    }
}

// =============================================================================
// Enhanced Call ID Utilities
// =============================================================================

/// Prefix for client-generated call IDs (when server doesn't provide one).
pub const CLIENT_GEN_PREFIX: &str = "cligen@";
/// Prefix for server-generated (enhanced) call IDs.
pub const SERVER_GEN_PREFIX: &str = "srvgen@";

/// Generate a client-side call_id with embedded function name and index.
///
/// Format: `cligen@<function_name>#<index>@<uuid>`
///
/// Use when the server doesn't provide a call_id for function calls.
/// The index allows distinguishing multiple calls to the same function.
pub fn generate_client_call_id(function_name: &str, index: usize) -> String {
    format!(
        "{}{}#{}@{}",
        CLIENT_GEN_PREFIX,
        function_name,
        index,
        uuid::Uuid::new_v4()
    )
}

/// Enhance a server-provided call_id with embedded function name.
///
/// Format: `srvgen@<function_name>@<original_call_id>`
///
/// Preserves the original call_id for later extraction when sending back to server.
pub fn enhance_server_call_id(original_id: &str, function_name: &str) -> String {
    format!("{}{}@{}", SERVER_GEN_PREFIX, function_name, original_id)
}

/// Check if a call_id was generated/enhanced by us (has cligen@ or srvgen@ prefix).
pub fn is_enhanced_call_id(call_id: &str) -> bool {
    call_id.starts_with(CLIENT_GEN_PREFIX) || call_id.starts_with(SERVER_GEN_PREFIX)
}

/// Check if a call_id is client-generated (cligen@ prefix).
pub fn is_client_generated_call_id(call_id: &str) -> bool {
    call_id.starts_with(CLIENT_GEN_PREFIX)
}

/// Parse function name from enhanced call_id (works for both cligen@ and srvgen@).
///
/// For client-generated: `cligen@<name>#<index>@<uuid>` → extracts `<name>`
/// For server-generated: `srvgen@<name>@<original_id>` → extracts `<name>`
///
/// Returns `None` for non-enhanced IDs.
pub fn parse_function_name_from_call_id(call_id: &str) -> Option<&str> {
    if let Some(rest) = call_id.strip_prefix(CLIENT_GEN_PREFIX) {
        // Client-generated format: <name>#<index>@<uuid>
        rest.split('#').next()
    } else if let Some(rest) = call_id.strip_prefix(SERVER_GEN_PREFIX) {
        // Server-generated format: <name>@<original_id>
        rest.split('@').next()
    } else {
        None
    }
}

/// Parse the index from a client-generated call_id.
///
/// Format: `cligen@<function_name>#<index>@<uuid>`
///
/// Returns `None` for server-generated or invalid format.
pub fn parse_call_index(call_id: &str) -> Option<usize> {
    let rest = call_id.strip_prefix(CLIENT_GEN_PREFIX)?;
    // Format: <function_name>#<index>@<uuid>
    let hash_pos = rest.find('#')?;
    let after_hash = &rest[hash_pos + 1..];
    let at_pos = after_hash.find('@')?;
    after_hash[..at_pos].parse().ok()
}

/// Extract original call_id from server-enhanced format.
///
/// Returns `None` for client-generated format.
pub fn extract_original_call_id(call_id: &str) -> Option<&str> {
    let rest = call_id.strip_prefix(SERVER_GEN_PREFIX)?;
    // Format: <function_name>@<original_id>
    rest.split('@').nth(1)
}

// =============================================================================
// ResponseItem Debug Logging
// =============================================================================

/// Log unexpected ResponseItem variant for debugging.
///
/// Use this in adapter `prompt_to_contents` catch-all branches to track
/// unexpected item types that don't match expected patterns.
///
/// # Arguments
/// * `item` - The ResponseItem that didn't match expected patterns
/// * `adapter_name` - Name of the adapter (e.g., "genai", "anthropic")
/// * `context` - Context where this occurred (e.g., "prompt_to_contents")
pub fn log_unexpected_response_item(item: &ResponseItem, adapter_name: &str, context: &str) {
    let (item_type, details) = match item {
        ResponseItem::Message { id, role, .. } => {
            ("Message", format!("role={}, has_id={}", role, id.is_some()))
        }
        ResponseItem::FunctionCall {
            id, name, call_id, ..
        } => (
            "FunctionCall",
            format!(
                "name={}, call_id={}, has_id={}",
                name,
                call_id,
                id.is_some()
            ),
        ),
        ResponseItem::FunctionCallOutput { call_id, .. } => {
            ("FunctionCallOutput", format!("call_id={}", call_id))
        }
        ResponseItem::Reasoning {
            id,
            encrypted_content,
            ..
        } => (
            "Reasoning",
            format!(
                "id={}, has_encrypted_content={}",
                id,
                encrypted_content.is_some()
            ),
        ),
        ResponseItem::LocalShellCall { .. } => ("LocalShellCall", String::new()),
        ResponseItem::CustomToolCall { .. } => ("CustomToolCall", String::new()),
        ResponseItem::CustomToolCallOutput { .. } => ("CustomToolCallOutput", String::new()),
        ResponseItem::WebSearchCall { .. } => ("WebSearchCall", String::new()),
        ResponseItem::GhostSnapshot { .. } => ("GhostSnapshot", String::new()),
        ResponseItem::Compaction { .. } => ("Compaction", String::new()),
        ResponseItem::Other => ("Other", String::new()),
    };

    tracing::warn!(
        adapter = adapter_name,
        context = context,
        item_type = item_type,
        details = details,
        "Unexpected ResponseItem variant"
    );
}

// =============================================================================
// Encrypted Content Storage Format
// =============================================================================

/// Provider SDK identifier for Google Generative AI.
pub const PROVIDER_SDK_GENAI: &str = "genai";
/// Provider SDK identifier for Volcengine Ark.
pub const PROVIDER_SDK_VOLCENGINE_ARK: &str = "volcengine-ark";
/// Provider SDK identifier for Anthropic.
pub const PROVIDER_SDK_ANTHROPIC: &str = "anthropic";
/// Provider SDK identifier for Z.AI.
pub const PROVIDER_SDK_ZAI: &str = "zai";
/// Provider SDK identifier for native OpenAI Responses API.
pub const PROVIDER_SDK_OPENAI: &str = "openai";

/// Prefix to identify our EncryptedContent JSON vs native API strings.
/// Native API may have its own encrypted_content (e.g., o1 reasoning),
/// so we use this prefix to distinguish our format.
pub const ENCRYPTED_CONTENT_PREFIX: &str = "codex-ec:";

/// Unified encrypted_content storage format.
///
/// Used by adapters to store full response body for round-trip preservation.
/// For adapters: stores raw HTTP response JSON.
/// For native OpenAI (SSE): stores NormalizedAssistantMessage JSON.
///
/// JSON format:
/// ```json
/// {
///   "_full_response_body": <raw_json or NormalizedAssistantMessage>,
///   "_provider_sdk": "genai" | "volcengine-ark" | "anthropic" | "zai" | "openai",
///   "_base_url": "https://...",
///   "_model": "model-name",
///   "_original_encrypted_content": "...",  // Optional, for OpenAI restore
///   "_ext": {}
/// }
/// ```
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EncryptedContent {
    /// Full response body from provider SDK (raw JSON) or NormalizedAssistantMessage (OpenAI SSE).
    #[serde(rename = "_full_response_body")]
    pub full_response_body: Value,

    /// Provider SDK identifier (e.g., "genai", "volcengine-ark", "openai").
    #[serde(rename = "_provider_sdk")]
    pub provider_sdk: String,

    /// Endpoint URL for model switch detection.
    #[serde(rename = "_base_url")]
    pub base_url: String,

    /// Model name for model switch detection.
    #[serde(rename = "_model")]
    pub model: String,

    /// Original encrypted_content from ResponseItem.Reasoning (preserved for restore on same-model sendback).
    #[serde(
        rename = "_original_encrypted_content",
        default,
        skip_serializing_if = "Option::is_none"
    )]
    pub original_encrypted_content: Option<String>,

    /// Extension data for future use.
    #[serde(rename = "_ext", default)]
    pub ext: Value,
}

impl EncryptedContent {
    /// Create new EncryptedContent with full context for model switch detection.
    pub fn new(
        full_response_body: Value,
        provider_sdk: impl Into<String>,
        base_url: impl Into<String>,
        model: impl Into<String>,
    ) -> Self {
        Self {
            full_response_body,
            provider_sdk: provider_sdk.into(),
            base_url: base_url.into(),
            model: model.into(),
            original_encrypted_content: None,
            ext: Value::Object(Default::default()),
        }
    }

    /// Create from raw body string (parses JSON) with full context.
    pub fn from_body_str(
        body: &str,
        provider_sdk: impl Into<String>,
        base_url: impl Into<String>,
        model: impl Into<String>,
    ) -> Option<Self> {
        let full_response_body: Value = serde_json::from_str(body).ok()?;
        Some(Self::new(full_response_body, provider_sdk, base_url, model))
    }

    /// Create with original_encrypted_content preserved (for OpenAI SSE).
    pub fn with_original_encrypted_content(
        full_response_body: Value,
        provider_sdk: impl Into<String>,
        base_url: impl Into<String>,
        model: impl Into<String>,
        original_encrypted_content: Option<String>,
    ) -> Self {
        Self {
            full_response_body,
            provider_sdk: provider_sdk.into(),
            base_url: base_url.into(),
            model: model.into(),
            original_encrypted_content,
            ext: Value::Object(Default::default()),
        }
    }

    /// Check if (base_url, model) matches current context.
    /// Used to determine if we can use fast-path (same model) or need normalization.
    pub fn matches_context(&self, base_url: &str, model: &str) -> bool {
        self.base_url == base_url && self.model == model
    }

    /// Serialize to JSON string with prefix for storage in encrypted_content field.
    /// Format: "codex-ec:{json}"
    pub fn to_json_string(&self) -> Option<String> {
        serde_json::to_string(self)
            .ok()
            .map(|json| format!("{ENCRYPTED_CONTENT_PREFIX}{json}"))
    }

    /// Deserialize from prefixed JSON string.
    /// Only parses strings that start with "codex-ec:" prefix.
    pub fn from_json_string(s: &str) -> Option<Self> {
        s.strip_prefix(ENCRYPTED_CONTENT_PREFIX)
            .and_then(|json| serde_json::from_str(json).ok())
    }

    /// Check if a string is our EncryptedContent format (has "codex-ec:" prefix).
    /// Use this to distinguish our format from native API's encrypted_content.
    pub fn is_codex_format(s: &str) -> bool {
        s.starts_with(ENCRYPTED_CONTENT_PREFIX)
    }

    /// Extract full_response_body as specific type.
    pub fn parse_body<T: for<'de> Deserialize<'de>>(&self) -> Option<T> {
        serde_json::from_value(self.full_response_body.clone()).ok()
    }

    /// Convert to NormalizedAssistantMessage by dispatching to provider-specific extractor.
    ///
    /// Used for cross-adapter model switching: when the stored response comes from a
    /// different adapter than the current one, extract as normalized format first,
    /// then convert to target adapter's message type.
    pub fn to_normalized(&self) -> Option<crate::normalized::NormalizedAssistantMessage> {
        match self.provider_sdk.as_str() {
            PROVIDER_SDK_GENAI => {
                crate::adapters::genai::convert::extract_normalized(&self.full_response_body)
            }
            PROVIDER_SDK_ANTHROPIC => {
                crate::adapters::anthropic::convert::extract_normalized(&self.full_response_body)
            }
            PROVIDER_SDK_VOLCENGINE_ARK => {
                crate::adapters::volcengine_ark::convert::extract_normalized(
                    &self.full_response_body,
                )
            }
            PROVIDER_SDK_ZAI => {
                crate::adapters::zai::convert::extract_normalized(&self.full_response_body)
            }
            PROVIDER_SDK_OPENAI => {
                crate::adapters::openai::convert::extract_normalized(&self.full_response_body)
            }
            _ => None,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use codex_protocol::models::FunctionCallOutputPayload;

    // =============================================================================
    // is_llm_generated tests
    // =============================================================================

    #[test]
    fn test_is_llm_generated_assistant_message() {
        let item = ResponseItem::Message {
            id: Some("msg_1".to_string()),
            role: "assistant".to_string(),
            content: vec![],
            end_turn: None,
        };
        assert!(is_llm_generated(&item));
    }

    #[test]
    fn test_is_llm_generated_user_message() {
        let item = ResponseItem::Message {
            id: None,
            role: "user".to_string(),
            content: vec![],
            end_turn: None,
        };
        assert!(!is_llm_generated(&item));
    }

    #[test]
    fn test_is_llm_generated_reasoning() {
        let item = ResponseItem::Reasoning {
            id: "rs_1".to_string(),
            summary: vec![],
            content: None,
            encrypted_content: None,
        };
        assert!(is_llm_generated(&item));
    }

    #[test]
    fn test_is_llm_generated_function_call() {
        let item = ResponseItem::FunctionCall {
            id: Some("fc_1".to_string()),
            name: "read_file".to_string(),
            arguments: "{}".to_string(),
            call_id: "call_1".to_string(),
        };
        assert!(is_llm_generated(&item));
    }

    #[test]
    fn test_is_llm_generated_function_call_output() {
        let item = ResponseItem::FunctionCallOutput {
            call_id: "call_1".to_string(),
            output: FunctionCallOutputPayload {
                content: "output".to_string(),
                content_items: None,
                success: Some(true),
            },
        };
        assert!(!is_llm_generated(&item));
    }

    // =============================================================================
    // filter_incremental_input tests
    // =============================================================================

    #[test]
    fn test_filter_incremental_input_first_turn() {
        let history = vec![ResponseItem::Message {
            id: None,
            role: "user".to_string(),
            content: vec![],
            end_turn: None,
        }];

        let filtered = filter_incremental_input(&history);
        assert!(filtered.is_none()); // No LLM items, first turn
    }

    #[test]
    fn test_filter_incremental_input_normal() {
        let history = vec![
            ResponseItem::Message {
                id: None,
                role: "user".to_string(),
                content: vec![],
                end_turn: None,
            },
            ResponseItem::Message {
                id: Some("msg_1".to_string()),
                role: "assistant".to_string(),
                content: vec![],
                end_turn: None,
            },
            ResponseItem::FunctionCall {
                id: Some("fc_1".to_string()),
                name: "tool".to_string(),
                arguments: "{}".to_string(),
                call_id: "call_1".to_string(),
            },
            ResponseItem::FunctionCallOutput {
                call_id: "call_1".to_string(),
                output: FunctionCallOutputPayload {
                    content: "result".to_string(),
                    content_items: None,
                    success: Some(true),
                },
            },
            ResponseItem::Message {
                id: None,
                role: "user".to_string(),
                content: vec![],
                end_turn: None,
            },
        ];

        let filtered = filter_incremental_input(&history);
        assert!(filtered.is_some());
        let filtered = filtered.unwrap();
        assert_eq!(filtered.len(), 2); // FunctionCallOutput + user message
    }

    #[test]
    fn test_filter_incremental_input_llm_last() {
        let history = vec![ResponseItem::Message {
            id: Some("msg_1".to_string()),
            role: "assistant".to_string(),
            content: vec![],
            end_turn: None,
        }];

        let filtered = filter_incremental_input(&history);
        assert!(filtered.is_some());
        assert_eq!(filtered.unwrap().len(), 0); // Empty slice
    }

    // =============================================================================
    // parse_complete_response tests
    // =============================================================================

    #[test]
    fn test_parse_complete_response_success() {
        let body = r#"{
            "id": "resp-123",
            "status": "completed",
            "output": [{
                "type": "message",
                "id": "msg-1",
                "role": "assistant",
                "content": [{"type": "output_text", "text": "Hello"}]
            }],
            "usage": {
                "input_tokens": 100,
                "output_tokens": 50,
                "input_tokens_details": {"cached_tokens": 20},
                "output_tokens_details": {"reasoning_tokens": 10}
            }
        }"#;

        let response = parse_complete_response(body).unwrap();
        assert_eq!(response.response_id, "resp-123");
        assert_eq!(response.events.len(), 2); // OutputItemDone + Completed
        assert!(response.token_usage.is_some());

        let usage = response.token_usage.unwrap();
        assert_eq!(usage.input_tokens, 100);
        assert_eq!(usage.output_tokens, 50);
        assert_eq!(usage.cached_input_tokens, 20);
        assert_eq!(usage.reasoning_output_tokens, 10);
    }

    #[test]
    fn test_parse_complete_response_failed() {
        let body = r#"{
            "id": "resp-123",
            "status": "failed",
            "error": {
                "code": "context_length_exceeded",
                "message": "Input too long"
            },
            "output": []
        }"#;

        let result = parse_complete_response(body);
        assert!(result.is_err());
        match result.unwrap_err() {
            ApiError::ContextWindowExceeded => {}
            other => panic!("Expected ContextWindowExceeded, got {:?}", other),
        }
    }

    #[test]
    fn test_parse_complete_response_previous_response_not_found() {
        let body = r#"{
            "id": "resp-123",
            "status": "failed",
            "error": {
                "code": "previous_response_not_found",
                "message": "Previous response ID not found"
            },
            "output": []
        }"#;

        let result = parse_complete_response(body);
        assert!(result.is_err());
        match result.unwrap_err() {
            ApiError::PreviousResponseNotFound => {}
            other => panic!("Expected PreviousResponseNotFound, got {:?}", other),
        }
    }

    #[test]
    fn test_parse_complete_response_incomplete() {
        let body = r#"{
            "id": "resp-123",
            "status": "incomplete",
            "incomplete_details": {"reason": "max_output_tokens"},
            "output": []
        }"#;

        let result = parse_complete_response(body);
        assert!(result.is_err());
        match result.unwrap_err() {
            ApiError::Stream(msg) => {
                assert!(msg.contains("incomplete"));
                assert!(msg.contains("max_output_tokens"));
            }
            other => panic!("Expected Stream error, got {:?}", other),
        }
    }

    #[test]
    fn test_parse_complete_response_missing_status() {
        let body = r#"{
            "id": "resp-123",
            "output": []
        }"#;

        let result = parse_complete_response(body);
        assert!(result.is_err());
        match result.unwrap_err() {
            ApiError::Stream(msg) => assert!(msg.contains("Missing status")),
            other => panic!("Expected Stream error, got {:?}", other),
        }
    }

    #[test]
    fn test_parse_output_item_function_call() {
        let item_data = serde_json::json!({
            "type": "function_call",
            "id": "fc_1",
            "name": "read_file",
            "call_id": "call_123",
            "arguments": "{\"path\": \"/test.txt\"}"
        });

        let result = parse_output_item(&item_data).unwrap();
        assert!(result.is_some());
        match result.unwrap() {
            ResponseItem::FunctionCall {
                id,
                name,
                call_id,
                arguments,
            } => {
                assert_eq!(id, Some("fc_1".to_string()));
                assert_eq!(name, "read_file");
                assert_eq!(call_id, "call_123");
                assert!(arguments.contains("path"));
            }
            _ => panic!("Expected FunctionCall"),
        }
    }

    #[test]
    fn test_parse_output_item_reasoning() {
        let item_data = serde_json::json!({
            "type": "reasoning",
            "id": "rs_1",
            "summary": [{"text": "Thinking about the problem"}],
            "content": [{"text": "Detailed reasoning..."}],
            "encrypted_content": "encrypted_data"
        });

        let result = parse_output_item(&item_data).unwrap();
        assert!(result.is_some());
        match result.unwrap() {
            ResponseItem::Reasoning {
                id,
                summary,
                content,
                encrypted_content,
            } => {
                assert_eq!(id, "rs_1");
                assert_eq!(summary.len(), 1);
                assert!(content.is_some());
                assert_eq!(encrypted_content, Some("encrypted_data".to_string()));
            }
            _ => panic!("Expected Reasoning"),
        }
    }

    // =============================================================================
    // Enhanced Call ID tests
    // =============================================================================

    #[test]
    fn test_generate_client_call_id() {
        let call_id = generate_client_call_id("get_weather", 0);
        assert!(call_id.starts_with("cligen@get_weather#0@"));
        assert!(is_enhanced_call_id(&call_id));
        assert!(is_client_generated_call_id(&call_id));
        assert_eq!(
            parse_function_name_from_call_id(&call_id),
            Some("get_weather")
        );
        assert_eq!(parse_call_index(&call_id), Some(0));
        assert_eq!(extract_original_call_id(&call_id), None);
    }

    #[test]
    fn test_generate_client_call_id_with_index() {
        let call_id_0 = generate_client_call_id("get_weather", 0);
        let call_id_1 = generate_client_call_id("get_weather", 1);
        let call_id_5 = generate_client_call_id("get_weather", 5);

        assert!(call_id_0.starts_with("cligen@get_weather#0@"));
        assert!(call_id_1.starts_with("cligen@get_weather#1@"));
        assert!(call_id_5.starts_with("cligen@get_weather#5@"));

        assert_eq!(parse_call_index(&call_id_0), Some(0));
        assert_eq!(parse_call_index(&call_id_1), Some(1));
        assert_eq!(parse_call_index(&call_id_5), Some(5));

        // All should have the same function name
        assert_eq!(
            parse_function_name_from_call_id(&call_id_0),
            Some("get_weather")
        );
        assert_eq!(
            parse_function_name_from_call_id(&call_id_1),
            Some("get_weather")
        );
        assert_eq!(
            parse_function_name_from_call_id(&call_id_5),
            Some("get_weather")
        );
    }

    #[test]
    fn test_parse_call_index_server_generated() {
        // Server-generated IDs don't have index
        let server_id = enhance_server_call_id("call_abc123", "search_files");
        assert_eq!(parse_call_index(&server_id), None);
    }

    #[test]
    fn test_enhance_server_call_id() {
        let call_id = enhance_server_call_id("call_abc123", "search_files");
        assert_eq!(call_id, "srvgen@search_files@call_abc123");
        assert!(is_enhanced_call_id(&call_id));
        assert!(!is_client_generated_call_id(&call_id));
        assert_eq!(
            parse_function_name_from_call_id(&call_id),
            Some("search_files")
        );
        assert_eq!(extract_original_call_id(&call_id), Some("call_abc123"));
    }

    #[test]
    fn test_non_enhanced_call_id() {
        let call_id = "some_random_call_id";
        assert!(!is_enhanced_call_id(call_id));
        assert!(!is_client_generated_call_id(call_id));
        assert_eq!(parse_function_name_from_call_id(call_id), None);
        assert_eq!(extract_original_call_id(call_id), None);
    }

    #[test]
    fn test_function_name_with_underscores() {
        // Function names with underscores should work correctly
        let client_id = generate_client_call_id("read_file_contents", 0);
        assert_eq!(
            parse_function_name_from_call_id(&client_id),
            Some("read_file_contents")
        );
        assert_eq!(parse_call_index(&client_id), Some(0));

        let server_id = enhance_server_call_id("srv_123", "write_to_database");
        assert_eq!(
            parse_function_name_from_call_id(&server_id),
            Some("write_to_database")
        );
        assert_eq!(extract_original_call_id(&server_id), Some("srv_123"));
    }

    // =============================================================================
    // EncryptedContent tests
    // =============================================================================

    #[test]
    fn test_encrypted_content_new() {
        let body = serde_json::json!({"key": "value"});
        let ec = EncryptedContent::new(
            body.clone(),
            PROVIDER_SDK_GENAI,
            "https://api.example.com",
            "gemini-2.0-flash",
        );

        assert_eq!(ec.full_response_body, body);
        assert_eq!(ec.provider_sdk, "genai");
        assert_eq!(ec.base_url, "https://api.example.com");
        assert_eq!(ec.model, "gemini-2.0-flash");
        assert!(ec.original_encrypted_content.is_none());
        assert!(ec.ext.is_object());
    }

    #[test]
    fn test_encrypted_content_from_body_str() {
        let body_str = r#"{"key": "value"}"#;
        let ec = EncryptedContent::from_body_str(
            body_str,
            PROVIDER_SDK_GENAI,
            "https://api.example.com",
            "gemini-2.0-flash",
        )
        .unwrap();

        assert_eq!(
            ec.full_response_body.get("key").unwrap().as_str().unwrap(),
            "value"
        );
        assert_eq!(ec.provider_sdk, "genai");
        assert_eq!(ec.base_url, "https://api.example.com");
        assert_eq!(ec.model, "gemini-2.0-flash");
    }

    #[test]
    fn test_encrypted_content_roundtrip() {
        let body = serde_json::json!({"test": 123});
        let ec = EncryptedContent::new(
            body,
            PROVIDER_SDK_VOLCENGINE_ARK,
            "https://ark.cn-beijing.volces.com",
            "doubao-pro",
        );

        let json_str = ec.to_json_string().unwrap();

        // Verify prefix is added
        assert!(json_str.starts_with(ENCRYPTED_CONTENT_PREFIX));
        assert!(EncryptedContent::is_codex_format(&json_str));

        let ec2 = EncryptedContent::from_json_string(&json_str).unwrap();

        assert_eq!(
            ec2.full_response_body
                .get("test")
                .unwrap()
                .as_i64()
                .unwrap(),
            123
        );
        assert_eq!(ec2.provider_sdk, "volcengine-ark");
        assert_eq!(ec2.base_url, "https://ark.cn-beijing.volces.com");
        assert_eq!(ec2.model, "doubao-pro");
    }

    #[test]
    fn test_encrypted_content_parse_body() {
        #[derive(serde::Deserialize, PartialEq, Debug)]
        struct TestBody {
            key: String,
        }

        let body = serde_json::json!({"key": "hello"});
        let ec = EncryptedContent::new(
            body,
            PROVIDER_SDK_GENAI,
            "https://api.example.com",
            "gemini-2.0-flash",
        );

        let parsed: TestBody = ec.parse_body().unwrap();
        assert_eq!(parsed.key, "hello");
    }

    #[test]
    fn test_encrypted_content_json_format() {
        let body = serde_json::json!({"data": 42});
        let ec = EncryptedContent::new(
            body,
            PROVIDER_SDK_GENAI,
            "https://api.example.com",
            "gemini-2.0-flash",
        );
        let json_str = ec.to_json_string().unwrap();

        // Verify prefix is present
        assert!(json_str.starts_with(ENCRYPTED_CONTENT_PREFIX));

        // Strip prefix and parse JSON
        let json_only = json_str.strip_prefix(ENCRYPTED_CONTENT_PREFIX).unwrap();
        let parsed: Value = serde_json::from_str(json_only).unwrap();
        assert!(parsed.get("_full_response_body").is_some());
        assert!(parsed.get("_provider_sdk").is_some());
        assert!(parsed.get("_base_url").is_some());
        assert!(parsed.get("_model").is_some());
        assert!(parsed.get("_ext").is_some());
        // _original_encrypted_content should be skipped when None
        assert!(parsed.get("_original_encrypted_content").is_none());
    }

    #[test]
    fn test_encrypted_content_is_codex_format() {
        // Our format
        let our_format = "codex-ec:{\"_full_response_body\":{}}";
        assert!(EncryptedContent::is_codex_format(our_format));

        // Native API format (e.g., o1 reasoning)
        let native_format = "eyJhbGciOiJkaXIiLCJlbmMiOiJBMjU2R0NNIn0";
        assert!(!EncryptedContent::is_codex_format(native_format));

        // Empty string
        assert!(!EncryptedContent::is_codex_format(""));

        // Random JSON (not our format)
        let random_json = r#"{"key": "value"}"#;
        assert!(!EncryptedContent::is_codex_format(random_json));
    }

    #[test]
    fn test_encrypted_content_from_json_string_rejects_non_prefixed() {
        // Direct JSON without prefix should not parse
        let direct_json = r#"{"_full_response_body":{},"_provider_sdk":"genai","_base_url":"","_model":"","_ext":{}}"#;
        assert!(EncryptedContent::from_json_string(direct_json).is_none());

        // With prefix should parse
        let prefixed = format!("{ENCRYPTED_CONTENT_PREFIX}{direct_json}");
        assert!(EncryptedContent::from_json_string(&prefixed).is_some());
    }

    #[test]
    fn test_encrypted_content_matches_context() {
        let body = serde_json::json!({"data": 42});
        let ec = EncryptedContent::new(
            body,
            PROVIDER_SDK_GENAI,
            "https://api.example.com",
            "gemini-2.0-flash",
        );

        // Same context
        assert!(ec.matches_context("https://api.example.com", "gemini-2.0-flash"));
        // Different model
        assert!(!ec.matches_context("https://api.example.com", "gemini-1.5-pro"));
        // Different base_url
        assert!(!ec.matches_context("https://other.api.com", "gemini-2.0-flash"));
        // Both different
        assert!(!ec.matches_context("https://other.api.com", "gpt-4o"));
    }

    #[test]
    fn test_encrypted_content_with_original() {
        let body = serde_json::json!({"text": "hello"});
        let original = Some("original_encrypted_data".to_string());
        let ec = EncryptedContent::with_original_encrypted_content(
            body,
            PROVIDER_SDK_OPENAI,
            "https://api.openai.com",
            "gpt-4o",
            original.clone(),
        );

        assert_eq!(ec.provider_sdk, "openai");
        assert_eq!(ec.original_encrypted_content, original);

        // Verify it serializes with prefix and roundtrips correctly
        let json_str = ec.to_json_string().unwrap();
        assert!(json_str.starts_with(ENCRYPTED_CONTENT_PREFIX));

        // Parse back and verify original_encrypted_content is preserved
        let parsed = EncryptedContent::from_json_string(&json_str).unwrap();
        assert_eq!(
            parsed.original_encrypted_content.as_deref(),
            Some("original_encrypted_data")
        );
    }
}
