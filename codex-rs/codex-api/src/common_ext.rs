//! Non-streaming response handling and tweakcc input filtering utilities.
//!
//! This module provides:
//! - `NonStreamingResponse` type for complete API responses
//! - Response parsing from JSON (based on gpt_adapter.rs)
//! - Incremental input filtering for `previous_response_id` support (based on item_utils.rs)

use crate::common::ResponseEvent;
use crate::error::ApiError;
use codex_protocol::models::ContentItem;
use codex_protocol::models::ReasoningItemContent;
use codex_protocol::models::ReasoningItemReasoningSummary;
use codex_protocol::models::ResponseItem;
use codex_protocol::protocol::TokenUsage;
use serde::Deserialize;
use serde_json::Value;
use tokio::sync::mpsc;

/// Complete response from non-streaming API call.
#[derive(Debug)]
pub struct NonStreamingResponse {
    /// Response events (OutputItemDone, Completed, etc.)
    pub events: Vec<ResponseEvent>,
    /// Response ID from the API
    pub response_id: String,
    /// Token usage statistics
    pub token_usage: Option<TokenUsage>,
}

impl NonStreamingResponse {
    /// Convert to `ResponseStream` for compatibility with streaming API consumers.
    pub fn into_stream(self) -> crate::common::ResponseStream {
        let (tx, rx) = mpsc::channel(self.events.len() + 1);
        tokio::spawn(async move {
            for event in self.events {
                let _ = tx.send(Ok(event)).await;
            }
        });
        crate::common::ResponseStream { rx_event: rx }
    }
}

// =============================================================================
// Incremental Input Filtering (from core/src/adapters/item_utils.rs)
// =============================================================================

/// Determine if a `ResponseItem` was generated by the LLM.
///
/// LLM-generated items include:
/// - Assistant messages
/// - Reasoning items
/// - Function calls (tool calls requested by the model)
/// - Custom tool calls
/// - Local shell calls
/// - Web search calls
///
/// User-provided items include:
/// - User messages
/// - Function call outputs (tool results)
/// - Custom tool call outputs
/// - Ghost snapshots
/// - Compaction summaries
pub fn is_llm_generated(item: &ResponseItem) -> bool {
    match item {
        // LLM outputs (server has via previous_response_id)
        ResponseItem::Message { role, .. } if role == "assistant" => true,
        ResponseItem::Reasoning { .. } => true,
        ResponseItem::FunctionCall { .. } => true,
        ResponseItem::CustomToolCall { .. } => true,
        ResponseItem::LocalShellCall { .. } => true,
        ResponseItem::WebSearchCall { .. } => true,

        // User inputs (server needs in request)
        ResponseItem::FunctionCallOutput { .. } => false,
        ResponseItem::CustomToolCallOutput { .. } => false,
        ResponseItem::Message { role, .. } if role == "user" => false,
        ResponseItem::GhostSnapshot { .. } => false,
        ResponseItem::Compaction { .. } => false,
        ResponseItem::Other => false,

        // Edge case: message with unknown role (defensive)
        ResponseItem::Message { .. } => false,
    }
}

/// Filter input to get items after last LLM-generated item (zero-copy).
///
/// Used for tweakcc mode when `previous_response_id` is present.
/// The server already has history up to the last LLM response, so we only
/// need to send user inputs that occurred after that point.
///
/// # Returns
///
/// - `None` - No LLM items found (first turn, caller should use full input)
/// - `Some(&[])` - LLM item is last (error state, caller should return error)
/// - `Some(&[...])` - Items after last LLM item (normal case)
///
/// # Example
///
/// ```ignore
/// match filter_incremental_input(&prompt.input) {
///     None => /* first turn, use full input */,
///     Some(slice) if slice.is_empty() => /* error: no user input after LLM */,
///     Some(slice) => /* use filtered slice */,
/// }
/// ```
pub fn filter_incremental_input(full_input: &[ResponseItem]) -> Option<&[ResponseItem]> {
    let last_llm_idx = full_input.iter().rposition(is_llm_generated)?;
    Some(&full_input[last_llm_idx + 1..])
}

// =============================================================================
// Response Parsing (from core/src/adapters/gpt_openapi/gpt_adapter.rs)
// =============================================================================

/// Token usage details for complete (non-streaming) responses.
#[derive(Debug, Deserialize)]
struct CompleteResponseUsage {
    input_tokens: i64,
    input_tokens_details: Option<CompleteResponseInputTokensDetails>,
    output_tokens: i64,
    output_tokens_details: Option<CompleteResponseOutputTokensDetails>,
}

/// Input token details for complete responses.
#[derive(Debug, Deserialize)]
struct CompleteResponseInputTokensDetails {
    cached_tokens: i64,
}

/// Output token details for complete responses.
#[derive(Debug, Deserialize)]
struct CompleteResponseOutputTokensDetails {
    reasoning_tokens: i64,
}

/// Parse complete (non-streaming) Responses API JSON response.
///
/// Converts a complete JSON response body to `NonStreamingResponse`.
/// Based on `gpt_adapter.rs::parse_complete_responses_json()`.
pub fn parse_complete_response(body: &str) -> Result<NonStreamingResponse, ApiError> {
    let data: Value =
        serde_json::from_str(body).map_err(|e| ApiError::Stream(format!("Invalid JSON: {e}")))?;

    let mut events = Vec::new();

    // Extract response ID
    let response_id = data
        .get("id")
        .and_then(|i| i.as_str())
        .unwrap_or("")
        .to_string();

    tracing::debug!(response_id = %response_id, "Parsing non-streaming response");

    // Check status field (must be present and valid)
    let Some(status) = data.get("status").and_then(|s| s.as_str()) else {
        return Err(ApiError::Stream("Missing status field in response".into()));
    };

    // Handle different status values
    match status {
        "completed" => {
            // Continue with normal processing
        }
        "failed" => {
            if let Some(error) = data.get("error") {
                return Err(classify_error(error));
            }
            return Err(ApiError::Stream(
                "Response failed without error details".into(),
            ));
        }
        "incomplete" => {
            let reason = data
                .get("incomplete_details")
                .and_then(|d| d.get("reason"))
                .and_then(|r| r.as_str())
                .unwrap_or("unknown");

            return Err(ApiError::Stream(format!("Response incomplete: {reason}")));
        }
        _ => {
            // Unknown status - reject it
            return Err(ApiError::Stream(format!(
                "Unknown response status: {status}"
            )));
        }
    }

    // Parse output items
    if let Some(output_array) = data.get("output").and_then(|o| o.as_array()) {
        for item_data in output_array {
            if let Some(item) = parse_output_item(item_data)? {
                events.push(ResponseEvent::OutputItemDone(item));
            }
        }
    }

    // Parse token usage with proper nested structure handling
    let token_usage = parse_token_usage(data.get("usage"));

    // Add Completed event
    events.push(ResponseEvent::Completed {
        response_id: response_id.clone(),
        token_usage: token_usage.clone(),
    });

    Ok(NonStreamingResponse {
        events,
        response_id,
        token_usage,
    })
}

/// Parse a single output item from complete response.
///
/// Returns `None` if item type is not recognized or parsing fails silently.
fn parse_output_item(item_data: &Value) -> Result<Option<ResponseItem>, ApiError> {
    tracing::debug!(
        item_data = ?item_data,
        "Parsing output item from response"
    );

    let item_type = item_data.get("type").and_then(|t| t.as_str()).unwrap_or("");

    match item_type {
        "message" => {
            let id = item_data
                .get("id")
                .and_then(|i| i.as_str())
                .map(std::string::ToString::to_string);

            let role = item_data
                .get("role")
                .and_then(|r| r.as_str())
                .unwrap_or("assistant")
                .to_string();

            // Parse content array
            let mut content = Vec::new();
            if let Some(content_array) = item_data.get("content").and_then(|c| c.as_array()) {
                for content_item in content_array {
                    let content_type = content_item
                        .get("type")
                        .and_then(|t| t.as_str())
                        .unwrap_or("");

                    match content_type {
                        "output_text" => {
                            if let Some(text) = content_item.get("text").and_then(|t| t.as_str()) {
                                content.push(ContentItem::OutputText {
                                    text: text.to_string(),
                                });
                            }
                        }
                        _ => {
                            // Skip unknown content types
                        }
                    }
                }
            }

            Ok(Some(ResponseItem::Message { id, role, content }))
        }

        "function_call" => {
            let id = item_data
                .get("id")
                .and_then(|i| i.as_str())
                .map(std::string::ToString::to_string);

            let name = item_data
                .get("name")
                .and_then(|n| n.as_str())
                .unwrap_or("")
                .to_string();

            let call_id = item_data
                .get("call_id")
                .and_then(|c| c.as_str())
                .unwrap_or("")
                .to_string();

            let arguments = item_data
                .get("arguments")
                .and_then(|a| a.as_str())
                .unwrap_or("{}")
                .to_string();

            Ok(Some(ResponseItem::FunctionCall {
                id,
                name,
                call_id,
                arguments,
            }))
        }

        "reasoning" => {
            let id = item_data
                .get("id")
                .and_then(|i| i.as_str())
                .unwrap_or("")
                .to_string();

            // Parse summary array
            let mut summary = Vec::new();
            if let Some(summary_array) = item_data.get("summary").and_then(|s| s.as_array()) {
                for summary_item in summary_array {
                    if let Some(text) = summary_item.get("text").and_then(|t| t.as_str()) {
                        summary.push(ReasoningItemReasoningSummary::SummaryText {
                            text: text.to_string(),
                        });
                    }
                }
            }

            // Parse content array (optional)
            let content =
                if let Some(content_array) = item_data.get("content").and_then(|c| c.as_array()) {
                    let mut content_items = Vec::new();
                    for content_item in content_array {
                        if let Some(text) = content_item.get("text").and_then(|t| t.as_str()) {
                            content_items.push(ReasoningItemContent::ReasoningText {
                                text: text.to_string(),
                            });
                        }
                    }
                    if content_items.is_empty() {
                        None
                    } else {
                        Some(content_items)
                    }
                } else {
                    None
                };

            let encrypted_content = item_data
                .get("encrypted_content")
                .and_then(|e| e.as_str())
                .map(std::string::ToString::to_string);

            Ok(Some(ResponseItem::Reasoning {
                id,
                summary,
                content,
                encrypted_content,
            }))
        }

        _ => {
            // Unknown item type, skip it
            Ok(None)
        }
    }
}

/// Parse token usage from response JSON.
fn parse_token_usage(usage: Option<&Value>) -> Option<TokenUsage> {
    usage.and_then(|u| {
        serde_json::from_value::<CompleteResponseUsage>(u.clone())
            .ok()
            .map(|usage| TokenUsage {
                input_tokens: usage.input_tokens,
                cached_input_tokens: usage
                    .input_tokens_details
                    .map(|d| d.cached_tokens)
                    .unwrap_or(0),
                output_tokens: usage.output_tokens,
                reasoning_output_tokens: usage
                    .output_tokens_details
                    .map(|d| d.reasoning_tokens)
                    .unwrap_or(0),
                total_tokens: usage.input_tokens + usage.output_tokens,
            })
    })
}

/// Classify error from response JSON into appropriate `ApiError`.
fn classify_error(error: &Value) -> ApiError {
    let code = error.get("code").and_then(|c| c.as_str()).unwrap_or("");
    let message = error
        .get("message")
        .and_then(|m| m.as_str())
        .unwrap_or("Unknown error");

    // Check message for encrypted content verification error
    // OpenAI returns: "The encrypted content for item rs_xxx could not be verified."
    if message.contains("encrypted content") && message.contains("could not be verified") {
        return ApiError::EncryptedContentInvalid;
    }

    match code {
        "context_length_exceeded" => ApiError::ContextWindowExceeded,
        "insufficient_quota" => ApiError::QuotaExceeded,
        "previous_response_not_found" => ApiError::PreviousResponseNotFound,
        _ => ApiError::Stream(message.to_string()),
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use codex_protocol::models::FunctionCallOutputPayload;

    // =============================================================================
    // is_llm_generated tests
    // =============================================================================

    #[test]
    fn test_is_llm_generated_assistant_message() {
        let item = ResponseItem::Message {
            id: Some("msg_1".to_string()),
            role: "assistant".to_string(),
            content: vec![],
        };
        assert!(is_llm_generated(&item));
    }

    #[test]
    fn test_is_llm_generated_user_message() {
        let item = ResponseItem::Message {
            id: None,
            role: "user".to_string(),
            content: vec![],
        };
        assert!(!is_llm_generated(&item));
    }

    #[test]
    fn test_is_llm_generated_reasoning() {
        let item = ResponseItem::Reasoning {
            id: "rs_1".to_string(),
            summary: vec![],
            content: None,
            encrypted_content: None,
        };
        assert!(is_llm_generated(&item));
    }

    #[test]
    fn test_is_llm_generated_function_call() {
        let item = ResponseItem::FunctionCall {
            id: Some("fc_1".to_string()),
            name: "read_file".to_string(),
            arguments: "{}".to_string(),
            call_id: "call_1".to_string(),
        };
        assert!(is_llm_generated(&item));
    }

    #[test]
    fn test_is_llm_generated_function_call_output() {
        let item = ResponseItem::FunctionCallOutput {
            call_id: "call_1".to_string(),
            output: FunctionCallOutputPayload {
                content: "output".to_string(),
                content_items: None,
                success: Some(true),
            },
        };
        assert!(!is_llm_generated(&item));
    }

    // =============================================================================
    // filter_incremental_input tests
    // =============================================================================

    #[test]
    fn test_filter_incremental_input_first_turn() {
        let history = vec![ResponseItem::Message {
            id: None,
            role: "user".to_string(),
            content: vec![],
        }];

        let filtered = filter_incremental_input(&history);
        assert!(filtered.is_none()); // No LLM items, first turn
    }

    #[test]
    fn test_filter_incremental_input_normal() {
        let history = vec![
            ResponseItem::Message {
                id: None,
                role: "user".to_string(),
                content: vec![],
            },
            ResponseItem::Message {
                id: Some("msg_1".to_string()),
                role: "assistant".to_string(),
                content: vec![],
            },
            ResponseItem::FunctionCall {
                id: Some("fc_1".to_string()),
                name: "tool".to_string(),
                arguments: "{}".to_string(),
                call_id: "call_1".to_string(),
            },
            ResponseItem::FunctionCallOutput {
                call_id: "call_1".to_string(),
                output: FunctionCallOutputPayload {
                    content: "result".to_string(),
                    content_items: None,
                    success: Some(true),
                },
            },
            ResponseItem::Message {
                id: None,
                role: "user".to_string(),
                content: vec![],
            },
        ];

        let filtered = filter_incremental_input(&history);
        assert!(filtered.is_some());
        let filtered = filtered.unwrap();
        assert_eq!(filtered.len(), 2); // FunctionCallOutput + user message
    }

    #[test]
    fn test_filter_incremental_input_llm_last() {
        let history = vec![ResponseItem::Message {
            id: Some("msg_1".to_string()),
            role: "assistant".to_string(),
            content: vec![],
        }];

        let filtered = filter_incremental_input(&history);
        assert!(filtered.is_some());
        assert_eq!(filtered.unwrap().len(), 0); // Empty slice
    }

    // =============================================================================
    // parse_complete_response tests
    // =============================================================================

    #[test]
    fn test_parse_complete_response_success() {
        let body = r#"{
            "id": "resp-123",
            "status": "completed",
            "output": [{
                "type": "message",
                "id": "msg-1",
                "role": "assistant",
                "content": [{"type": "output_text", "text": "Hello"}]
            }],
            "usage": {
                "input_tokens": 100,
                "output_tokens": 50,
                "input_tokens_details": {"cached_tokens": 20},
                "output_tokens_details": {"reasoning_tokens": 10}
            }
        }"#;

        let response = parse_complete_response(body).unwrap();
        assert_eq!(response.response_id, "resp-123");
        assert_eq!(response.events.len(), 2); // OutputItemDone + Completed
        assert!(response.token_usage.is_some());

        let usage = response.token_usage.unwrap();
        assert_eq!(usage.input_tokens, 100);
        assert_eq!(usage.output_tokens, 50);
        assert_eq!(usage.cached_input_tokens, 20);
        assert_eq!(usage.reasoning_output_tokens, 10);
    }

    #[test]
    fn test_parse_complete_response_failed() {
        let body = r#"{
            "id": "resp-123",
            "status": "failed",
            "error": {
                "code": "context_length_exceeded",
                "message": "Input too long"
            },
            "output": []
        }"#;

        let result = parse_complete_response(body);
        assert!(result.is_err());
        match result.unwrap_err() {
            ApiError::ContextWindowExceeded => {}
            other => panic!("Expected ContextWindowExceeded, got {:?}", other),
        }
    }

    #[test]
    fn test_parse_complete_response_previous_response_not_found() {
        let body = r#"{
            "id": "resp-123",
            "status": "failed",
            "error": {
                "code": "previous_response_not_found",
                "message": "Previous response ID not found"
            },
            "output": []
        }"#;

        let result = parse_complete_response(body);
        assert!(result.is_err());
        match result.unwrap_err() {
            ApiError::PreviousResponseNotFound => {}
            other => panic!("Expected PreviousResponseNotFound, got {:?}", other),
        }
    }

    #[test]
    fn test_parse_complete_response_incomplete() {
        let body = r#"{
            "id": "resp-123",
            "status": "incomplete",
            "incomplete_details": {"reason": "max_output_tokens"},
            "output": []
        }"#;

        let result = parse_complete_response(body);
        assert!(result.is_err());
        match result.unwrap_err() {
            ApiError::Stream(msg) => {
                assert!(msg.contains("incomplete"));
                assert!(msg.contains("max_output_tokens"));
            }
            other => panic!("Expected Stream error, got {:?}", other),
        }
    }

    #[test]
    fn test_parse_complete_response_missing_status() {
        let body = r#"{
            "id": "resp-123",
            "output": []
        }"#;

        let result = parse_complete_response(body);
        assert!(result.is_err());
        match result.unwrap_err() {
            ApiError::Stream(msg) => assert!(msg.contains("Missing status")),
            other => panic!("Expected Stream error, got {:?}", other),
        }
    }

    #[test]
    fn test_parse_output_item_function_call() {
        let item_data = serde_json::json!({
            "type": "function_call",
            "id": "fc_1",
            "name": "read_file",
            "call_id": "call_123",
            "arguments": "{\"path\": \"/test.txt\"}"
        });

        let result = parse_output_item(&item_data).unwrap();
        assert!(result.is_some());
        match result.unwrap() {
            ResponseItem::FunctionCall {
                id,
                name,
                call_id,
                arguments,
            } => {
                assert_eq!(id, Some("fc_1".to_string()));
                assert_eq!(name, "read_file");
                assert_eq!(call_id, "call_123");
                assert!(arguments.contains("path"));
            }
            _ => panic!("Expected FunctionCall"),
        }
    }

    #[test]
    fn test_parse_output_item_reasoning() {
        let item_data = serde_json::json!({
            "type": "reasoning",
            "id": "rs_1",
            "summary": [{"text": "Thinking about the problem"}],
            "content": [{"text": "Detailed reasoning..."}],
            "encrypted_content": "encrypted_data"
        });

        let result = parse_output_item(&item_data).unwrap();
        assert!(result.is_some());
        match result.unwrap() {
            ResponseItem::Reasoning {
                id,
                summary,
                content,
                encrypted_content,
            } => {
                assert_eq!(id, "rs_1");
                assert_eq!(summary.len(), 1);
                assert!(content.is_some());
                assert_eq!(encrypted_content, Some("encrypted_data".to_string()));
            }
            _ => panic!("Expected Reasoning"),
        }
    }
}
