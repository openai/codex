# ä¸º codex-rs çš„å®ç°å‚è€ƒ

## å¿«é€Ÿå†³ç­–çŸ©é˜µ

| æ¨¡å— | æ¨èåšæ³• | å·¥ä½œé‡ | ä¼˜å…ˆçº§ |
|------|--------|-------|-------|
| **Tantivy ç´¢å¼•å­˜å‚¨** | âœ“ ç›´æ¥å¤ç”¨ | ä½ | ğŸ”´ P0 |
| **Tree-sitter AST** | âœ“ å¤ç”¨ + è‡ªå®šä¹‰ | ä¸­ | ğŸ”´ P0 |
| **ä»£ç åˆ†å—** | âœ“ å¤ç”¨ TextSplitter | ä½ | ğŸ”´ P0 |
| **Embedding é›†æˆ** | âš ï¸ é€‚é…å™¨æ¨¡å¼ | ä½ | ğŸ”´ P0 |
| **Git åŒæ­¥** | âœ“ å¤ç”¨ git2 | ä½ | ğŸŸ¡ P1 |
| **å¹¶å‘æ¡†æ¶** | âœ“ å¤ç”¨ tokio | ä½ | ğŸ”´ P0 |
| **æ–‡æ¡£ç´¢å¼•** | âš ï¸ å¯é€‰æ¨¡å— | ä¸­ | ğŸŸ¢ P2 |
| **Rust ç‰¹å®šåˆ†æ** | ğŸ”¨ è‡ªå®ç° | é«˜ | ğŸŸ¢ P2 |

---

## Phase 1: MVP (æœ€å°å¯ç”¨äº§å“)

### ç›®æ ‡

ä¸º codex-rs å®ç°åŸºç¡€çš„ä»£ç ç´¢å¼•èƒ½åŠ›ï¼Œæ”¯æŒï¼š
- Rust ä»£ç ç´¢å¼•
- åŸºç¡€çš„ Python/TypeScript æ”¯æŒ
- å…³é”®è¯æœç´¢ + å‘é‡æœç´¢
- å¢é‡æ›´æ–°

### å®ç°æ­¥éª¤

#### æ­¥éª¤ 1: é¡¹ç›®ç»“æ„è®¾ç½® (1-2 å¤©)
```
Cargo.toml additions:
[dependencies]
tantivy = "0.21"
tree-sitter-tags = "0.22"
tree-sitter-rust = "0.21"
tree-sitter-python = "0.21"
tree-sitter-typescript = "0.21"
text-splitter = { version = "0.13", features = ["code"] }
```

#### æ­¥éª¤ 2: æ ¸å¿ƒæ•°æ®ç»“æ„ (1-2 å¤©)

```rust

use anyhow::Result;
use serde::{Deserialize, Serialize};

/// ç´¢å¼•é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IndexConfig {
    /// ç´¢å¼•å­˜å‚¨è·¯å¾„
    pub index_dir: PathBuf,

    /// æ”¯æŒçš„è¯­è¨€
    pub languages: Vec<String>,

    /// Embedding ç»´åº¦
    pub embedding_dim: usize,

    /// æœ€å¤§ chunk å¤§å°
    pub chunk_size: usize,
}

/// ä»£ç ç´¢å¼•ä¸»æ¥å£
pub struct CodeIndex {
    config: IndexConfig,
    tantivy_index: TantivyIndex,
    embedding_service: Arc<dyn EmbeddingService>,
}

impl CodeIndex {
    /// åˆ›å»ºæ–°ç´¢å¼•æˆ–æ‰“å¼€ç°æœ‰ç´¢å¼•
    pub async fn open_or_create(
        config: IndexConfig,
        embedding_service: Arc<dyn EmbeddingService>,
    ) -> Result<Self> {
        // 1. åˆ›å»ºæˆ–æ‰“å¼€ Tantivy ç´¢å¼•
        let tantivy_index = TantivyIndex::open_or_create(&config.index_dir)?;

        // 2. åˆå§‹åŒ–è¯­è¨€é…ç½®
        init_languages(&config.languages)?;

        Ok(CodeIndex {
            config,
            tantivy_index,
            embedding_service,
        })
    }

    /// ç´¢å¼•å•ä¸ªä»“åº“
    pub async fn index_repository(
        &self,
        repo_path: &Path,
        repo_id: &str,
    ) -> Result<IndexStats> {
        // ä» Tabby-Index çš„ CodeIndexer::refresh() æ”¹ç¼–
        // å…·ä½“å®ç°è§ä¸‹ä¸€èŠ‚
        todo!()
    }

    /// æœç´¢ä»£ç 
    pub async fn search(
        &self,
        query: &str,
        limit: usize,
    ) -> Result<Vec<SearchResult>> {
        // 1. å…³é”®è¯æœç´¢ (BM25)
        let text_results = self.tantivy_index.search_text(query, limit)?;

        // 2. å‘é‡æœç´¢ (å¯é€‰)
        let vector_results = if let Ok(embedding) =
            self.embedding_service.embed(query).await {
            self.tantivy_index.search_embedding(&embedding, limit)?
        } else {
            vec![]
        };

        // 3. èåˆç»“æœ
        Ok(fuse_results(&text_results, &vector_results, limit))
    }
}

pub struct IndexStats {
    pub indexed_files: usize,
    pub updated_files: usize,
    pub total_chunks: usize,
    pub elapsed_secs: f64,
}

pub struct SearchResult {
    pub file_path: String,
    pub language: String,
    pub chunk: String,
    pub start_line: i32,
    pub end_line: i32,
    pub score: f32,
}
```

#### æ­¥éª¤ 3: Tantivy ç´¢å¼•å°è£… (2-3 å¤©)

```rust

use tantivy::{Index, IndexReader, IndexWriter, Schema, doc};

pub struct TantivyIndex {
    index: Index,
    reader: IndexReader,
    writer: IndexWriter,
    schema: Schema,
}

impl TantivyIndex {
    /// æ‰“å¼€æˆ–åˆ›å»º Tantivy ç´¢å¼•
    pub fn open_or_create(index_path: &Path) -> Result<Self> {
        // 1. å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»º
        std::fs::create_dir_all(index_path)?;

        // 2. æ„å»º schema
        let schema = Self::build_schema();

        // 3. æ‰“å¼€æˆ–åˆ›å»ºç´¢å¼•
        let index = if index_path.join("meta.json").exists() {
            Index::open_in_dir(index_path)?
        } else {
            Index::create_in_dir(index_path, schema.clone())?
        };

        // 4. è·å–è¯»å†™å™¨
        let reader = index.reader()?;
        let writer = index.writer(5_000_000)?;  // 5MB ç¼“å†²

        Ok(TantivyIndex {
            index,
            reader,
            writer,
            schema,
        })
    }

    /// æ„å»º schema (ä» Tabby-Index æ”¹ç¼–)
    fn build_schema() -> Schema {
        let mut builder = tantivy::schema::Schema::builder();

        // åŸºç¡€å­—æ®µ
        builder.add_text_field("file_id", tantivy::schema::TEXT | tantivy::schema::STORED);
        builder.add_text_field("source_id", tantivy::schema::TEXT);
        builder.add_text_field("corpus", tantivy::schema::TEXT);
        builder.add_text_field("attributes", tantivy::schema::STRING | tantivy::schema::STORED);
        builder.add_i64_field("updated_at", tantivy::schema::INDEXED);

        // Chunk å­—æ®µ
        builder.add_text_field("chunk_id", tantivy::schema::TEXT);
        builder.add_text_field("chunk_attributes", tantivy::schema::STRING | tantivy::schema::STORED);
        builder.add_text_field("chunk_tokens", tantivy::schema::TEXT);
        builder.add_bytes_field("chunk_embedding", tantivy::schema::STORED);

        // ä»£ç ç‰¹å®šå­—æ®µ
        builder.add_text_field("filepath", tantivy::schema::TEXT);
        builder.add_text_field("language", tantivy::schema::TEXT);
        builder.add_text_field("commit", tantivy::schema::TEXT);
        builder.add_i64_field("start_line", tantivy::schema::INDEXED);
        builder.add_text_field("body", tantivy::schema::STRING | tantivy::schema::STORED);

        builder.build()
    }

    /// æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•
    pub fn add_document(
        &self,
        file_id: &str,
        source_id: &str,
        filepath: &str,
        language: &str,
        body: &str,
        chunk_tokens: &[String],
        embedding: Option<&[f32]>,
    ) -> Result<()> {
        use tantivy::schema::Value;

        let doc = doc!(
            self.schema.get_field("file_id")? => file_id,
            self.schema.get_field("source_id")? => source_id,
            self.schema.get_field("corpus")? => "code",
            self.schema.get_field("filepath")? => filepath,
            self.schema.get_field("language")? => language,
            self.schema.get_field("body")? => body,
            self.schema.get_field("chunk_tokens")? => chunk_tokens.join(" "),
            self.schema.get_field("updated_at")? => chrono::Utc::now().timestamp(),
        );

        if let Some(emb) = embedding {
            let binary = binarize_embedding(emb);
            // å‘ doc æ·»åŠ  embedding (éœ€è¦æ‰©å±•)
            // self.writer.add_document(doc)?;
        } else {
            self.writer.add_document(doc)?;
        }

        Ok(())
    }

    /// æäº¤å˜æ›´
    pub fn commit(&self) -> Result<()> {
        self.writer.commit()?;
        Ok(())
    }

    /// æœç´¢æ–‡æœ¬ (BM25)
    pub fn search_text(
        &self,
        query_text: &str,
        limit: usize,
    ) -> Result<Vec<SearchResult>> {
        use tantivy::query::QueryParser;

        let query_parser = QueryParser::for_index(
            &self.index,
            vec![self.schema.get_field("chunk_tokens")?],
        );

        let query = query_parser.parse_query(query_text)?;
        let searcher = self.reader.searcher();
        let top_docs = searcher.search(&query, &tantivy::collector::TopDocs::with_limit(limit))?;

        let mut results = vec![];
        for (_score, doc_address) in top_docs {
            let doc = searcher.doc(doc_address)?;
            results.push(SearchResult {
                file_path: doc.get_first("filepath")
                    .and_then(|v| v.as_text())
                    .unwrap_or("")
                    .to_string(),
                language: doc.get_first("language")
                    .and_then(|v| v.as_text())
                    .unwrap_or("")
                    .to_string(),
                chunk: doc.get_first("body")
                    .and_then(|v| v.as_text())
                    .unwrap_or("")
                    .to_string(),
                start_line: 0,  // TODO: ä» chunk_attributes è§£æ
                end_line: 0,
                score: _score,
            });
        }

        Ok(results)
    }

    /// æœç´¢å‘é‡
    pub fn search_embedding(
        &self,
        embedding: &[f32],
        limit: usize,
    ) -> Result<Vec<SearchResult>> {
        // TODO: å®ç°å‘é‡æœç´¢
        // æ–¹æ¡ˆ A: ä½¿ç”¨ Tantivy çš„å‘é‡åŠŸèƒ½ (0.22+)
        // æ–¹æ¡ˆ B: åå¤„ç† (å–å‡ºæ‰€æœ‰å‘é‡ï¼Œåœ¨å†…å­˜ä¸­è®¡ç®—)
        // æ–¹æ¡ˆ C: é›†æˆä¸“é—¨çš„å‘é‡åº“ (qdrant, milvus)

        todo!("å‘é‡æœç´¢å®ç°")
    }
}

fn binarize_embedding(embedding: &[f32]) -> Vec<u8> {
    // å°†æµ®ç‚¹å‘é‡è½¬æ¢ä¸ºå­—èŠ‚è¡¨ç¤º
    embedding
        .iter()
        .flat_map(|f| f.to_le_bytes().to_vec())
        .collect()
}
```

#### æ­¥éª¤ 4: AST å’Œè¯­è¨€æ”¯æŒ (2-3 å¤©)

```rust

use tree_sitter_tags::{Tags, TagsContext};
use std::collections::HashMap;

pub struct CodeIntelligence;

impl CodeIntelligence {
    /// æå–ä»£ç æ ‡ç­¾
    pub fn extract_tags(
        language: &str,
        content: &str,
    ) -> anyhow::Result<Vec<Tag>> {
        let config = get_language_config(language)?;

        // ä½¿ç”¨ tree-sitter-tags
        let mut cursor = TagsContext::new(
            config.language,
            content.as_bytes(),
            config.query.as_str(),
        )?;

        let mut tags = vec![];
        while let Some((name, start_point, end_point)) = cursor.next() {
            tags.push(Tag {
                name: name.to_string(),
                start_line: start_point.row as i32,
                start_column: start_point.column as i32,
                end_line: end_point.row as i32,
                end_column: end_point.column as i32,
                syntax_type: detect_syntax_type(name),
            });
        }

        Ok(tags)
    }

    /// è®¡ç®—ä»£ç æŒ‡æ ‡ (æœ‰æ•ˆæ€§æ£€æŸ¥)
    pub fn compute_metrics(content: &str) -> CodeMetrics {
        let lines: Vec<&str> = content.lines().collect();
        let total_chars = content.len();

        let max_line_length = lines.iter()
            .map(|l| l.len())
            .max()
            .unwrap_or(0) as i32;

        let avg_line_length = if !lines.is_empty() {
            total_chars as f32 / lines.len() as f32
        } else {
            0.0
        };

        let alphanum_count: usize = content
            .chars()
            .filter(|c| c.is_alphanumeric())
            .count();

        let alphanum_fraction = if total_chars > 0 {
            alphanum_count as f32 / total_chars as f32
        } else {
            0.0
        };

        CodeMetrics {
            max_line_length,
            avg_line_length,
            alphanum_fraction,
            num_lines: lines.len() as i32,
        }
    }

    /// æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æºä»£ç æ–‡ä»¶
    pub fn is_valid_file(metrics: &CodeMetrics) -> bool {
        metrics.max_line_length <= 300
            && metrics.avg_line_length <= 150.0
            && metrics.alphanum_fraction >= 0.25
            && metrics.num_lines <= 100000
    }

    /// ä»£ç åˆ†å— (ä» text-splitter æ”¹ç¼–)
    pub fn chunk_code(
        content: &str,
        _language: &str,
        chunk_size: usize,
    ) -> anyhow::Result<Vec<CodeChunk>> {
        use text_splitter::TextSplitter;

        let splitter = TextSplitter::new(chunk_size);
        let mut chunks = vec![];

        for chunk_text in splitter.split_text(content) {
            let start_line = count_lines_before(content, chunk_text) as i32;
            let end_line = start_line + chunk_text.lines().count() as i32 - 1;

            chunks.push(CodeChunk {
                text: chunk_text.to_string(),
                start_line,
                end_line,
            });
        }

        Ok(chunks)
    }
}

pub struct Tag {
    pub name: String,
    pub start_line: i32,
    pub start_column: i32,
    pub end_line: i32,
    pub end_column: i32,
    pub syntax_type: String,
}

pub struct CodeChunk {
    pub text: String,
    pub start_line: i32,
    pub end_line: i32,
}

pub struct CodeMetrics {
    pub max_line_length: i32,
    pub avg_line_length: f32,
    pub alphanum_fraction: f32,
    pub num_lines: i32,
}

fn detect_syntax_type(tag_name: &str) -> String {
    // æ ¹æ® tree-sitter-tags çš„è¾“å‡ºæ£€æµ‹è¯­æ³•ç±»å‹
    if tag_name.contains("function") {
        "function".to_string()
    } else if tag_name.contains("class") {
        "class".to_string()
    } else if tag_name.contains("struct") {
        "struct".to_string()
    } else {
        "definition".to_string()
    }
}

fn count_lines_before(content: &str, chunk: &str) -> usize {
    if let Some(pos) = content.find(chunk) {
        content[..pos].lines().count()
    } else {
        0
    }
}
```

#### æ­¥éª¤ 5: Embedding é›†æˆ (1-2 å¤©)

```rust

pub trait EmbeddingService: Send + Sync {
    /// ç”Ÿæˆä»£ç ç‰‡æ®µçš„ embedding
    async fn embed(&self, text: &str) -> anyhow::Result<Vec<f32>>;

    /// åµŒå…¥ç»´åº¦
    fn embedding_dim(&self) -> usize;
}

// é›†æˆåˆ° codex-rs çš„ inference æœåŠ¡
impl EmbeddingService for CodexEmbeddingAdapter {
    async fn embed(&self, text: &str) -> anyhow::Result<Vec<f32>> {
        // è°ƒç”¨ codex çš„ embedding æ¨¡å‹
        self.inference_service
            .embed(text, "code")  // æŒ‡å®š"code" embedding ä¸Šä¸‹æ–‡
            .await
    }

    fn embedding_dim(&self) -> usize {
        1536  // æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´
    }
}
```

---

## Phase 2: åŠŸèƒ½æ‰©å±• (å¯é€‰)

### ç›®æ ‡

- [ ] æ”¯æŒæ›´å¤šç¼–ç¨‹è¯­è¨€ (C++, Java, Go, etc.)
- [ ] æ–‡æ¡£ç´¢å¼• (Markdown, HTML)
- [ ] å‘é‡æœç´¢ä¼˜åŒ–
- [ ] æ··åˆæ’åºå’Œé‡æ’
- [ ] ç¼“å­˜å±‚ä¼˜åŒ–

### å®ç°å»ºè®®

#### 1. å¤šè¯­è¨€æ”¯æŒæ‰©å±•

```rust
// æ·»åŠ è¯­è¨€æ”¯æŒçš„æ­¥éª¤:

1. åœ¨ Cargo.toml ä¸­æ·»åŠ ä¾èµ–:
   tree-sitter-cpp = "0.21"
   tree-sitter-go = "0.21"
   tree-sitter-java = "0.21"

2. åœ¨ languages.rs ä¸­æ³¨å†Œ:
   LANGUAGE_CONFIGS.insert("cpp", create_cpp_config()?);
   LANGUAGE_CONFIGS.insert("go", create_go_config()?);
   LANGUAGE_CONFIGS.insert("java", create_java_config()?);

3. æ·»åŠ è¯­è¨€æ£€æµ‹ (åŸºäºæ–‡ä»¶æ‰©å±•å):
   fn detect_language(path: &Path) -> Option<&str> {
       path.extension()
           .and_then(|ext| ext.to_str())
           .and_then(|ext| LANGUAGE_MAP.get(ext).copied())
   }
```

#### 2. å‘é‡æœç´¢ä¼˜åŒ–

```rust
// å®ç°æ›´å¥½çš„å‘é‡æ£€ç´¢

pub struct HybridSearch {
    text_weight: f32,
    vector_weight: f32,
}

impl HybridSearch {
    pub async fn search(
        &self,
        query: &str,
        embedding: &[f32],
        index: &CodeIndex,
        limit: usize,
    ) -> Result<Vec<SearchResult>> {
        // 1. BM25 æœç´¢
        let text_results = index.search_text(query, limit * 2)?;

        // 2. å‘é‡æœç´¢
        let vector_results = index.search_embedding(embedding, limit * 2)?;

        // 3. åˆ†æ•°èåˆ (RRF: Reciprocal Rank Fusion)
        let mut combined = HashMap::new();

        for (i, result) in text_results.iter().enumerate() {
            combined.entry(result.file_path.clone())
                .or_insert(0.0)
                += self.text_weight / (i as f32 + 1.0);
        }

        for (i, result) in vector_results.iter().enumerate() {
            combined.entry(result.file_path.clone())
                .or_insert(0.0)
                += self.vector_weight / (i as f32 + 1.0);
        }

        // 4. æ’åºå’Œè¿”å›
        let mut results: Vec<_> = combined.into_iter().collect();
        results.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());

        Ok(results.into_iter()
            .take(limit)
            .map(|(path, score)| SearchResult {
                file_path: path,
                score,
                ..Default::default()
            })
            .collect())
    }
}
```

---

## Phase 3: Rust ç‰¹å®šä¼˜åŒ– (å¯é€‰)

### Rust ç‰¹å®šçš„ä»£ç åˆ†æ

```rust
pub struct RustCodeIntelligence;

impl RustCodeIntelligence {
    /// æå– Rust ç‰¹å®šçš„ä¿¡æ¯
    pub fn analyze_rust(content: &str) -> Result<RustAnalysis> {
        let mut analysis = RustAnalysis::default();

        // 1. æå– trait å®ç°
        analysis.trait_impls = Self::extract_trait_impls(content)?;

        // 2. æå– macro è°ƒç”¨
        analysis.macro_calls = Self::extract_macros(content)?;

        // 3. æå–ä¾èµ– (Cargo.toml)
        // analysis.dependencies = Self::extract_dependencies(content)?;

        // 4. æ ‡è¯† unsafe å—
        analysis.unsafe_blocks = Self::find_unsafe_blocks(content)?;

        Ok(analysis)
    }

    fn extract_trait_impls(content: &str) -> Result<Vec<TraitImpl>> {
        // ä½¿ç”¨ tree-sitter çš„ Rust parser
        // æŸ¥è¯¢æ¨¡å¼: (impl_item (trait_type (type_identifier)) @trait)
        todo!()
    }

    fn extract_macros(content: &str) -> Result<Vec<MacroCall>> {
        // æŸ¥è¯¢: (macro_invocation (identifier) @macro)
        todo!()
    }

    fn find_unsafe_blocks(content: &str) -> Result<Vec<UnsafeBlock>> {
        // æŸ¥è¯¢: (unsafe_block) @unsafe
        todo!()
    }
}

pub struct RustAnalysis {
    pub trait_impls: Vec<TraitImpl>,
    pub macro_calls: Vec<MacroCall>,
    pub unsafe_blocks: Vec<UnsafeBlock>,
}

pub struct TraitImpl {
    pub trait_name: String,
    pub impl_type: String,
    pub methods: Vec<String>,
}

pub struct MacroCall {
    pub name: String,
    pub args: String,
    pub line: i32,
}

pub struct UnsafeBlock {
    pub reason: Option<String>,
    pub line: i32,
}
```

---

## é›†æˆæ£€æŸ¥æ¸…å•

### âœ“ å¿…é¡»å®Œæˆçš„ä»»åŠ¡

- [ ] Tantivy ç´¢å¼•é›†æˆæµ‹è¯•
- [ ] Tree-sitter è¯­è¨€é…ç½®éªŒè¯
- [ ] Embedding æœåŠ¡é€‚é…å™¨
- [ ] åŸºæœ¬åŠŸèƒ½æµ‹è¯• (index + search)
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] é”™è¯¯å¤„ç†å’Œæ¢å¤
- [ ] æ—¥å¿—å’Œç›‘æ§é›†æˆ

### âš ï¸ éœ€è¦é…ç½®çš„ä»»åŠ¡

- [ ] ç´¢å¼•ç›®å½•é…ç½® (å¯é…ç½®è·¯å¾„)
- [ ] æ”¯æŒçš„è¯­è¨€åˆ—è¡¨ (config.toml)
- [ ] Chunk å¤§å°å‚æ•° (é»˜è®¤ 512 å­—ç¬¦)
- [ ] Embedding æœåŠ¡ URL
- [ ] æœç´¢æƒé‡å‚æ•° (BM25 vs Vector)

### ğŸš€ å¯é€‰ä¼˜åŒ–ä»»åŠ¡

- [ ] å¤šè¯­è¨€æ”¯æŒæ‰©å±•
- [ ] å‘é‡æœç´¢ä¼˜åŒ–
- [ ] ç¼“å­˜å±‚å®ç°
- [ ] Rust ç‰¹å®šåˆ†æ
- [ ] æ€§èƒ½è°ƒä¼˜ (å¹¶å‘åº¦ã€ç¼“å†²å¤§å°)

---

## å¸¸è§é—®é¢˜å’Œé™·é˜±

### Q1: Embedding ç»´åº¦å’Œæ¨¡å‹é€‰æ‹©?

**A**:
- é»˜è®¤å»ºè®®: 1536 ç»´åº¦ (OpenAI ada-002 standard)
- Tabby ä½¿ç”¨: å¯å˜ç»´åº¦ (æ”¯æŒå¤šæ¨¡å‹)
- codex-rs: ä½¿ç”¨ç°æœ‰ embedding æœåŠ¡çš„ç»´åº¦

### Q2: ç´¢å¼•æ›´æ–°é¢‘ç‡?

**A**:
- **å¼€å‘åœºæ™¯**: å®æ—¶æ›´æ–° (æ¯æ¬¡æ–‡ä»¶ä¿å­˜)
- **åˆ†æåœºæ™¯**: å®šæœŸæ›´æ–° (æ¯å°æ—¶æˆ–æ¯å¤©)
- **ç”Ÿäº§åœºæ™¯**: å¢é‡æ›´æ–° (Git webhook)

### Q3: ç´¢å¼•å¤§å°ä¼šå¾ˆå¤§å—?

**A**:
- **å­˜å‚¨**: ~150-200% æºä»£ç å¤§å°
- **1 GB ä»£ç ** â†’ ~1.5-2 GB ç´¢å¼•
- **ä¼˜åŒ–**: é€‰æ‹©æ€§å­—æ®µå­˜å‚¨ã€æ®µå‹ç¼©

### Q4: å¹¶å‘æ€§èƒ½å¦‚ä½•?

**A**:
- **å•çº¿ç¨‹**: ~1000 QPS (BM25)
- **å¤šçº¿ç¨‹** (16 cores): ~5000 QPS
- **Tantivy**: æ”¯æŒæ— é”è¯»å– (MVCC)

### Q5: å¦‚ä½•å¤„ç†å¤§å‹ä»“åº“ (>1GB)?

**A**:
```rust
// åˆ†å—å¤„ç†ç­–ç•¥:
for batch in file_tree.chunks(100) {
    process_batch(batch).await?;
    // æ¯æ‰¹åæäº¤
}

// æˆ–åˆ†åº“ç´¢å¼•:
let repos = split_by_language(&repo_path);
for sub_repo in repos {
    index_repository(sub_repo).await?;
}
```

---

## å‚è€ƒèµ„æº

### æ–‡æ¡£é“¾æ¥

- **Tantivy æ–‡æ¡£**: https://docs.rs/tantivy
- **Tree-sitter**: https://tree-sitter.github.io
- **Tree-sitter Rust**: https://tree-sitter.github.io/tree-sitter/references
- **Text Splitter**: https://docs.rs/text-splitter

### ä»£ç ç¤ºä¾‹

- **Tabby Index æºç **: https://github.com/TabbyML/tabby/tree/main/crates/tabby-index

---

## æ€»ç»“

### âœ“ ç«‹å³å¯ä»¥åš

1. é›†æˆ Tantivy å’Œ Tree-sitter (ä½æˆæœ¬, é«˜ä»·å€¼)
2. å®ç°åŸºç¡€çš„ä»£ç ç´¢å¼• (MVP ç›®æ ‡)
3. æ”¯æŒ Rustã€Pythonã€TypeScript (æœ€å¸¸è§è¯­è¨€)
4. æ•´åˆç°æœ‰çš„ embedding æœåŠ¡

### âš ï¸ éœ€è¦æ…é‡è€ƒè™‘

1. **Embedding æ¨¡å‹é€‰æ‹©**: ç¡®ä¿ä¸ codex-rs ä¸€è‡´
2. **ç´¢å¼•æ›´æ–°ç­–ç•¥**: å®æ—¶ vs ç¦»çº¿ (æ€§èƒ½æƒè¡¡)
3. **å‘é‡æœç´¢å®ç°**: Tantivy å†…ç½® vs ä¸“é—¨åº“
4. **ç¼“å­˜å’Œå†…å­˜**: å¤§å‹ç´¢å¼•çš„å†…å­˜å‹åŠ›

### ğŸš€ é•¿æœŸä¼˜åŒ–

1. **å¤šè¯­è¨€æ”¯æŒ**: é€æ­¥æ‰©å±•åˆ° 15+ è¯­è¨€
2. **å‘é‡æœç´¢ä¼˜åŒ–**: HNSWã€ANN ç´¢å¼•
3. **Rust ç‰¹å®šåˆ†æ**: macroã€traitã€unsafe åˆ†æ
4. **æ€§èƒ½è°ƒä¼˜**: åŸºå‡†æµ‹è¯•å’Œç“¶é¢ˆåˆ†æ

---

**ç›¸å…³æ–‡æ¡£**ï¼š
- [ç³»ç»Ÿæ¶æ„](./architecture.md)
- [æ ¸å¿ƒæ¨¡å—è¯¦è§£](./modules.md)
- [AST å’Œè¯­è¨€å¤„ç†](./ast-languages.md)
- [ç´¢å¼•æ„å»ºæµç¨‹](./indexing-process.md)
